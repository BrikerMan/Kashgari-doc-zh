{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kashgari Tip \u8be5\u6587\u6863\u53ea\u5305\u62ec Kashgari 1.x\uff0cKashgari 2.x \u8bf7\u9605\u8bfb https://kashgari.readthedocs.io/en/v2.0.0/ Kashgari \u662f\u4e00\u4e2a\u6781\u7b80\u4e14\u5f3a\u5927\u7684 NLP \u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u548c\u6807\u6ce8\u7684\u5b66\u4e60\uff0c\u7814\u7a76\u53ca\u90e8\u7f72\u4e0a\u7ebf\u3002 \u65b9\u4fbf\u6613\u7528 Kashgari \u63d0\u4f9b\u4e86\u7b80\u6d01\u7edf\u4e00\u7684 API \u548c\u5b8c\u5584\u7684\u6587\u6863\uff0c\u4f7f\u5176\u975e\u5e38\u65b9\u4fbf\u6613\u7528\u3002 \u5185\u7f6e\u8fc1\u79fb\u5b66\u4e60\u6a21\u5757 Kashgari \u901a\u8fc7\u63d0\u4f9b BertEmbedding , GPT2Embedding \uff0c WordEmbedding \u7b49\u7279\u5f81\u63d0\u53d6\u7c7b\uff0c\u65b9\u4fbf\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8fc1\u79fb\u5b66\u4e60\u3002 \u6613\u6269\u5c55 Kashgari \u63d0\u4f9b\u7b80\u4fbf\u7684\u63a5\u53e3\u548c\u7ee7\u627f\u5173\u7cfb\uff0c\u81ea\u884c\u6269\u5c55\u65b0\u7684\u6a21\u578b\u7ed3\u6784\u975e\u5e38\u65b9\u4fbf\u3002 \u53ef\u7528\u4e8e\u751f\u4ea7 \u901a\u8fc7\u628a Kashgari \u6a21\u578b\u5bfc\u51fa\u4e3a SavedModel \u683c\u5f0f\uff0c\u53ef\u4ee5\u4f7f\u7528 TensorFlow Serving \u6a21\u5757\u63d0\u4f9b\u670d\u52a1\uff0c\u76f4\u63a5\u5728\u7ebf\u4e0a\u73af\u5883\u4f7f\u7528\u3002 \u6211\u4eec\u7684\u4f7f\u547d # \u4e3a \u5b66\u672f\u7814\u7a76\u8005 \u63d0\u4f9b\u6613\u4e8e\u5b9e\u9a8c\u7684\u73af\u5883\uff0c\u53ef\u5feb\u901f\u9a8c\u8bc1\u7406\u8bba\u3002 \u4e3a NLP\u521d\u5b66\u8005 \u63d0\u4f9b\u6613\u4e8e\u5b66\u4e60\u6a21\u4eff\u7684\u751f\u4ea7\u7ea7\u522b\u5de5\u7a0b\u3002 \u4e3a NLP\u5de5\u4f5c\u8005 \u63d0\u4f9b\u5feb\u901f\u642d\u5efa\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u7b80\u5316\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\u3002 \u6559\u7a0b # \u8fd9\u662f\u4e00\u4e9b\u8be6\u7ec6\u7684\u6559\u7a0b: \u6559\u7a0b 1: \u6587\u672c\u5206\u7c7b \u6559\u7a0b 2: \u6587\u672c\u6807\u6ce8 \u8fd8\u6709\u4e00\u4e9b\u535a\u5ba2\u6587\u7ae0\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Kashgari: 15 \u5206\u949f\u642d\u5efa\u4e2d\u6587\u6587\u672c\u5206\u7c7b\u6a21\u578b \u57fa\u4e8e BERT \u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER) BERT/ERNIE \u6587\u672c\u5206\u7c7b\u548c\u90e8\u7f72 \u4e94\u5206\u949f\u642d\u5efa\u4e00\u4e2a\u57fa\u4e8eBERT\u7684NER\u6a21\u578b Multi-Class Text Classification with Kashgari in 15 minutes \u5feb\u901f\u5f00\u59cb # \u5b89\u88c5 # Important tf.keras \u7248\u672c pypi \u5305\u91cd\u547d\u540d\u4e3a kashgari-tf \u8be5\u9879\u76ee\u57fa\u4e8e Tensorflow 1.14.0 \u548c Python 3.6+. pip install kashgari-tf # CPU pip install tensorflow == 1 .14.0 # GPU pip install tensorflow-gpu == 1 .14.0 \u57fa\u7840\u7528\u6cd5 # \u4e0b\u9762\u6211\u4eec\u7528 Bi_LSTM \u6a21\u578b\u5b9e\u73b0\u4e00\u4e2a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\uff1a from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model # \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6\uff0c\u6b64\u5904\u53ef\u4ee5\u66ff\u6362\u6210\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u4fdd\u8bc1\u683c\u5f0f\u4e00\u81f4\u5373\u53ef train_x , train_y = ChineseDailyNerCorpus . load_data ( 'train' ) test_x , test_y = ChineseDailyNerCorpus . load_data ( 'test' ) valid_x , valid_y = ChineseDailyNerCorpus . load_data ( 'valid' ) model = BiLSTM_Model () model . fit ( train_x , train_y , valid_x , valid_y , epochs = 50 ) \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (None, 97) 0 _________________________________________________________________ layer_embedding (Embedding) (None, 97, 100) 320600 _________________________________________________________________ layer_blstm (Bidirectional) (None, 97, 256) 235520 _________________________________________________________________ layer_dropout (Dropout) (None, 97, 256) 0 _________________________________________________________________ layer_time_distributed (Time (None, 97, 8) 2056 _________________________________________________________________ activation_7 (Activation) (None, 97, 8) 0 ================================================================= Total params: 558,176 Trainable params: 558,176 Non-trainable params: 0 _________________________________________________________________ Train on 20864 samples, validate on 2318 samples Epoch 1/50 20864/20864 [==============================] - 9s 417us/sample - loss: 0.2508 - acc: 0.9333 - val_loss: 0.1240 - val_acc: 0.9607 \"\"\" \u4f7f\u7528 Bert \u8bed\u8a00\u6a21\u578b # from kashgari.embeddings import BERTEmbedding from kashgari.tasks.labeling import BiGRU_Model from kashgari.corpus import ChineseDailyNerCorpus # \u6b64\u5904\u9700\u8981\u81ea\u884c\u4e0b\u8f7d BERT \u6743\u91cd bert_embedding = BERTEmbedding ( '<bert-model-folder>' , sequence_length = 30 ) model = BiGRU_Model ( bert_embedding ) train_x , train_y = ChineseDailyNerCorpus . load_data () model . fit ( train_x , train_y ) \u6027\u80fd\u6307\u6807 # \u4efb\u52a1 \u8bed\u8a00 \u6570\u636e\u96c6 \u5f97\u5206 \u8be6\u60c5 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u4e2d\u6587 \u4eba\u6c11\u65e5\u62a5\u6570\u636e\u96c6 94.46 (F1) Text Labeling Performance Report \u8d21\u732e # \u5982\u679c\u5bf9 Kashgari \u611f\u5174\u8da3\uff0c\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u52a0\u5165\u5230\u8be5\u9879\u76ee\u3002\u53ef\u4ee5\u901a\u8fc7\u67e5\u9605 \u8d21\u732e\u6307\u5357 \u6765\u4e86\u89e3\u66f4\u591a\u3002","title":"\u4ecb\u7ecd"},{"location":"#_1","text":"\u4e3a \u5b66\u672f\u7814\u7a76\u8005 \u63d0\u4f9b\u6613\u4e8e\u5b9e\u9a8c\u7684\u73af\u5883\uff0c\u53ef\u5feb\u901f\u9a8c\u8bc1\u7406\u8bba\u3002 \u4e3a NLP\u521d\u5b66\u8005 \u63d0\u4f9b\u6613\u4e8e\u5b66\u4e60\u6a21\u4eff\u7684\u751f\u4ea7\u7ea7\u522b\u5de5\u7a0b\u3002 \u4e3a NLP\u5de5\u4f5c\u8005 \u63d0\u4f9b\u5feb\u901f\u642d\u5efa\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u7b80\u5316\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\u3002","title":"\u6211\u4eec\u7684\u4f7f\u547d"},{"location":"#_2","text":"\u8fd9\u662f\u4e00\u4e9b\u8be6\u7ec6\u7684\u6559\u7a0b: \u6559\u7a0b 1: \u6587\u672c\u5206\u7c7b \u6559\u7a0b 2: \u6587\u672c\u6807\u6ce8 \u8fd8\u6709\u4e00\u4e9b\u535a\u5ba2\u6587\u7ae0\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Kashgari: 15 \u5206\u949f\u642d\u5efa\u4e2d\u6587\u6587\u672c\u5206\u7c7b\u6a21\u578b \u57fa\u4e8e BERT \u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER) BERT/ERNIE \u6587\u672c\u5206\u7c7b\u548c\u90e8\u7f72 \u4e94\u5206\u949f\u642d\u5efa\u4e00\u4e2a\u57fa\u4e8eBERT\u7684NER\u6a21\u578b Multi-Class Text Classification with Kashgari in 15 minutes","title":"\u6559\u7a0b"},{"location":"#_3","text":"","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"#_4","text":"Important tf.keras \u7248\u672c pypi \u5305\u91cd\u547d\u540d\u4e3a kashgari-tf \u8be5\u9879\u76ee\u57fa\u4e8e Tensorflow 1.14.0 \u548c Python 3.6+. pip install kashgari-tf # CPU pip install tensorflow == 1 .14.0 # GPU pip install tensorflow-gpu == 1 .14.0","title":"\u5b89\u88c5"},{"location":"#_5","text":"\u4e0b\u9762\u6211\u4eec\u7528 Bi_LSTM \u6a21\u578b\u5b9e\u73b0\u4e00\u4e2a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\uff1a from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model # \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6\uff0c\u6b64\u5904\u53ef\u4ee5\u66ff\u6362\u6210\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u4fdd\u8bc1\u683c\u5f0f\u4e00\u81f4\u5373\u53ef train_x , train_y = ChineseDailyNerCorpus . load_data ( 'train' ) test_x , test_y = ChineseDailyNerCorpus . load_data ( 'test' ) valid_x , valid_y = ChineseDailyNerCorpus . load_data ( 'valid' ) model = BiLSTM_Model () model . fit ( train_x , train_y , valid_x , valid_y , epochs = 50 ) \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) (None, 97) 0 _________________________________________________________________ layer_embedding (Embedding) (None, 97, 100) 320600 _________________________________________________________________ layer_blstm (Bidirectional) (None, 97, 256) 235520 _________________________________________________________________ layer_dropout (Dropout) (None, 97, 256) 0 _________________________________________________________________ layer_time_distributed (Time (None, 97, 8) 2056 _________________________________________________________________ activation_7 (Activation) (None, 97, 8) 0 ================================================================= Total params: 558,176 Trainable params: 558,176 Non-trainable params: 0 _________________________________________________________________ Train on 20864 samples, validate on 2318 samples Epoch 1/50 20864/20864 [==============================] - 9s 417us/sample - loss: 0.2508 - acc: 0.9333 - val_loss: 0.1240 - val_acc: 0.9607 \"\"\"","title":"\u57fa\u7840\u7528\u6cd5"},{"location":"#bert","text":"from kashgari.embeddings import BERTEmbedding from kashgari.tasks.labeling import BiGRU_Model from kashgari.corpus import ChineseDailyNerCorpus # \u6b64\u5904\u9700\u8981\u81ea\u884c\u4e0b\u8f7d BERT \u6743\u91cd bert_embedding = BERTEmbedding ( '<bert-model-folder>' , sequence_length = 30 ) model = BiGRU_Model ( bert_embedding ) train_x , train_y = ChineseDailyNerCorpus . load_data () model . fit ( train_x , train_y )","title":"\u4f7f\u7528 Bert \u8bed\u8a00\u6a21\u578b"},{"location":"#_6","text":"\u4efb\u52a1 \u8bed\u8a00 \u6570\u636e\u96c6 \u5f97\u5206 \u8be6\u60c5 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u4e2d\u6587 \u4eba\u6c11\u65e5\u62a5\u6570\u636e\u96c6 94.46 (F1) Text Labeling Performance Report","title":"\u6027\u80fd\u6307\u6807"},{"location":"#_7","text":"\u5982\u679c\u5bf9 Kashgari \u611f\u5174\u8da3\uff0c\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u52a0\u5165\u5230\u8be5\u9879\u76ee\u3002\u53ef\u4ee5\u901a\u8fc7\u67e5\u9605 \u8d21\u732e\u6307\u5357 \u6765\u4e86\u89e3\u66f4\u591a\u3002","title":"\u8d21\u732e"},{"location":"FAQ/","text":"FAQ # How can I run Keras on GPU # Kashgari will use GPU by default if available, but you need to setup the Tensorflow GPU environment first. You can check gpu status using the code below: import tensorflow as tf print ( tf . test . is_gpu_available ()) Here is the official document of TensorFlow-GPU How to save and resume training with ModelCheckpoint callback # You can use tf.keras.callbacks.ModelCheckpoint for saving model during training. from tensorflow.python.keras.callbacks import ModelCheckpoint filepath = \"saved-model- {epoch:02d} - {acc:.2f} .hdf5\" checkpoint_callback = ModelCheckpoint ( filepath , monitor = 'acc' , verbose = 1 ) model = CNN_GRU_Model () model . fit ( train_x , train_y , valid_x , valid_y , callbacks = [ checkpoint_callback ]) ModelCheckpoint will save models struct and weights to target file, but we need token dict and label dict to fully restore the model, so we have to save model using model.save() function. So, the full solution will be like this. from tensorflow.python.keras.callbacks import ModelCheckpoint filepath = \"saved-model- {epoch:02d} - {acc:.2f} .hdf5\" checkpoint_callback = ModelCheckpoint ( filepath , monitor = 'acc' , verbose = 1 ) model = CNN_GRU_Model () # This function will build token dict, label dict and model struct. model . build_model ( train_x , train_y , valid_x , valid_y ) # Save full model info and initial weights to the full_model folder. model . save ( 'full_model' ) # Start Training model . fit ( train_x , train_y , valid_x , valid_y , callbacks = [ checkpoint_callback ]) # Load Model from kashgari.utils import load_model # We only need model struct and dicts new_model = load_model ( 'full_model' , load_weights = False ) # Load weights from ModelCheckpoint new_model . tf_model . load_weights ( 'saved-model-05-0.96.hdf5' ) # Resume Training # Only need to set {'initial_epoch': 5} when you wish to start new epoch from 6 # Otherwise epoch will start from 1 model . fit ( train_x , train_y , valid_x , valid_y , callbacks = [ checkpoint_callback ], epochs = 10 , fit_kwargs = { 'initial_epoch' : 5 })","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#how-can-i-run-keras-on-gpu","text":"Kashgari will use GPU by default if available, but you need to setup the Tensorflow GPU environment first. You can check gpu status using the code below: import tensorflow as tf print ( tf . test . is_gpu_available ()) Here is the official document of TensorFlow-GPU","title":"How can I run Keras on GPU"},{"location":"FAQ/#how-to-save-and-resume-training-with-modelcheckpoint-callback","text":"You can use tf.keras.callbacks.ModelCheckpoint for saving model during training. from tensorflow.python.keras.callbacks import ModelCheckpoint filepath = \"saved-model- {epoch:02d} - {acc:.2f} .hdf5\" checkpoint_callback = ModelCheckpoint ( filepath , monitor = 'acc' , verbose = 1 ) model = CNN_GRU_Model () model . fit ( train_x , train_y , valid_x , valid_y , callbacks = [ checkpoint_callback ]) ModelCheckpoint will save models struct and weights to target file, but we need token dict and label dict to fully restore the model, so we have to save model using model.save() function. So, the full solution will be like this. from tensorflow.python.keras.callbacks import ModelCheckpoint filepath = \"saved-model- {epoch:02d} - {acc:.2f} .hdf5\" checkpoint_callback = ModelCheckpoint ( filepath , monitor = 'acc' , verbose = 1 ) model = CNN_GRU_Model () # This function will build token dict, label dict and model struct. model . build_model ( train_x , train_y , valid_x , valid_y ) # Save full model info and initial weights to the full_model folder. model . save ( 'full_model' ) # Start Training model . fit ( train_x , train_y , valid_x , valid_y , callbacks = [ checkpoint_callback ]) # Load Model from kashgari.utils import load_model # We only need model struct and dicts new_model = load_model ( 'full_model' , load_weights = False ) # Load weights from ModelCheckpoint new_model . tf_model . load_weights ( 'saved-model-05-0.96.hdf5' ) # Resume Training # Only need to set {'initial_epoch': 5} when you wish to start new epoch from 6 # Otherwise epoch will start from 1 model . fit ( train_x , train_y , valid_x , valid_y , callbacks = [ checkpoint_callback ], epochs = 10 , fit_kwargs = { 'initial_epoch' : 5 })","title":"How to save and resume training with ModelCheckpoint callback"},{"location":"about/contributing/","text":"Contributing & Support # We are happy to accept your contributions to make Kashgari better and more awesome! You could contribute in various ways: Bug Reports # Please read the documentation and search the issue tracker to try and find the answer to your question before posting an issue. When creating an issue on the repository, please provide as much info as possible: Version being used. Operating system. Version of Python. Errors in console. Detailed description of the problem. Examples for reproducing the error. You can post pictures, but if specific text or code is required to reproduce the issue, please provide the text in a plain text format for easy copy/paste. The more info provided the greater the chance someone will take the time to answer, implement, or fix the issue. Be prepared to answer questions and provide additional information if required. Issues in which the creator refuses to respond to follow up questions will be marked as stale and closed. Reviewing Code # Take part in reviewing pull requests and/or reviewing direct commits. Make suggestions to improve the code and discuss solutions to overcome weakness in the algorithm. Answer Questions in Issues # Take time and answer questions and offer suggestions to people who've created issues in the issue tracker. Often people will have questions that you might have an answer for. Or maybe you know how to help them accomplish a specific task they are asking about. Feel free to share your experience to help others out. Pull Requests # Pull requests are welcome, and a great way to help fix bugs and add new features. Accuracy Benchmarks # Use Kashgari your own data, and report the F-1 score. Adding New Models # New models can be of two basic types: Adding New Tasks # Currently, Kashgari can handle text-classification and sequence-labeling tasks. If you want to apply Kashgari for a new task, please submit a request issue and explain why we would consider adding the new task to Kashgari Documentation Improvements # A ton of time has been spent not only creating and supporting this tool, but also spent making this documentation. If you feel it is still lacking, show your appreciation for the tool by helping to improve/translate the documentation.","title":"Contributing &amp; Support"},{"location":"about/contributing/#contributing-support","text":"We are happy to accept your contributions to make Kashgari better and more awesome! You could contribute in various ways:","title":"Contributing &amp; Support"},{"location":"about/contributing/#bug-reports","text":"Please read the documentation and search the issue tracker to try and find the answer to your question before posting an issue. When creating an issue on the repository, please provide as much info as possible: Version being used. Operating system. Version of Python. Errors in console. Detailed description of the problem. Examples for reproducing the error. You can post pictures, but if specific text or code is required to reproduce the issue, please provide the text in a plain text format for easy copy/paste. The more info provided the greater the chance someone will take the time to answer, implement, or fix the issue. Be prepared to answer questions and provide additional information if required. Issues in which the creator refuses to respond to follow up questions will be marked as stale and closed.","title":"Bug Reports"},{"location":"about/contributing/#reviewing-code","text":"Take part in reviewing pull requests and/or reviewing direct commits. Make suggestions to improve the code and discuss solutions to overcome weakness in the algorithm.","title":"Reviewing Code"},{"location":"about/contributing/#answer-questions-in-issues","text":"Take time and answer questions and offer suggestions to people who've created issues in the issue tracker. Often people will have questions that you might have an answer for. Or maybe you know how to help them accomplish a specific task they are asking about. Feel free to share your experience to help others out.","title":"Answer Questions in Issues"},{"location":"about/contributing/#pull-requests","text":"Pull requests are welcome, and a great way to help fix bugs and add new features.","title":"Pull Requests"},{"location":"about/contributing/#accuracy-benchmarks","text":"Use Kashgari your own data, and report the F-1 score.","title":"Accuracy Benchmarks"},{"location":"about/contributing/#adding-new-models","text":"New models can be of two basic types:","title":"Adding New Models"},{"location":"about/contributing/#adding-new-tasks","text":"Currently, Kashgari can handle text-classification and sequence-labeling tasks. If you want to apply Kashgari for a new task, please submit a request issue and explain why we would consider adding the new task to Kashgari","title":"Adding New Tasks"},{"location":"about/contributing/#documentation-improvements","text":"A ton of time has been spent not only creating and supporting this tool, but also spent making this documentation. If you feel it is still lacking, show your appreciation for the tool by helping to improve/translate the documentation.","title":"Documentation Improvements"},{"location":"about/release-notes/","text":"Release notes # Upgrading # To upgrade Material to the latest version, use pip : pip install --upgrade kashgari-tf To inspect the currently installed version, use the following command: pip show kashgari-tf Current Release # 0.5.2 - 2019.08.10 # \ud83d\udca5 Add CuDNN Cell config, disable auto CuDNN cell. ( #182 , #198 ) 0.5.1 - 2019.07.15 # \ud83d\udcdd Rewrite documents with mkdocs \ud83d\udcdd Add Chinese documents \u2728 Add predict_top_k_class for classification model to get predict probabilities ( #146 ) \ud83d\udeb8 Add label2idx , token2idx properties to Embeddings and Models \ud83d\udeb8 Add tokenizer property for BERT Embedding. ( #136 ) \ud83d\udeb8 Add predict_kwargs for models predict() function \u26a1\ufe0f Change multi-label classification's default loss function to binary_crossentropy ( #151 ) 0.5.0 - 2019.07.11 # \ud83c\udf89\ud83c\udf89 tf.keras version \ud83c\udf89\ud83c\udf89 \ud83c\udf89 Rewrite Kashgari using tf.keras ( #77 ) \ud83c\udf89 Rewrite Documents \u2728 Add TPU support \u2728 Add TF-Serving support. \u2728 Add advance customization support, like multi-input model \ud83d\udc0e Performance optimization Legacy Version Changelog # 0.2.6 - 2019.07.12 # \ud83d\udcdd Add tf.keras version info \ud83d\udc1b Fixing lstm issue in labeling model ( #125 ) 0.2.4 - 2019.06.06 # Add BERT output feature layer fine-tune support. Discussion: ( #103 ) Add BERT output feature layer number selection, default 4 according to BERT paper Fix BERT embedding token index offset issue ( #104 0.2.1 - 2019.03.05 # fix missing sequence_labeling_tokenize_add_bos_eos config 0.2.0 # multi-label classification for all classification models support cuDNN cell for sequence labeling add option for output BOS and EOS in sequence labeling result, fix #31 0.1.9 # add AVCNNModel , KMaxCNNModel , RCNNModel , AVRNNModel , DropoutBGRUModel , DropoutAVRNNModel model to classification task. fix several small bugs 0.1.8 # fix BERT Embedding model's to_json function, issue #19 0.1.7 # remove class candidates filter to fix #16 overwrite init function in CustomEmbedding add parameter check to custom_embedding layer add keras-bert version to setup.py file 0.1.6 # add output_dict , debug_info params to text_classification model add output_dict , debug_info and chunk_joiner params to text_classification model fix possible crash at data_generator 0.1.5 # fix sequence labeling evaluate result output refactor model save and load function 0.1.4 # fix classification model evaluate result output change test settings","title":"Release notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release notes"},{"location":"about/release-notes/#upgrading","text":"To upgrade Material to the latest version, use pip : pip install --upgrade kashgari-tf To inspect the currently installed version, use the following command: pip show kashgari-tf","title":"Upgrading"},{"location":"about/release-notes/#current-release","text":"","title":"Current Release"},{"location":"about/release-notes/#052-20190810","text":"\ud83d\udca5 Add CuDNN Cell config, disable auto CuDNN cell. ( #182 , #198 )","title":"0.5.2 - 2019.08.10"},{"location":"about/release-notes/#051-20190715","text":"\ud83d\udcdd Rewrite documents with mkdocs \ud83d\udcdd Add Chinese documents \u2728 Add predict_top_k_class for classification model to get predict probabilities ( #146 ) \ud83d\udeb8 Add label2idx , token2idx properties to Embeddings and Models \ud83d\udeb8 Add tokenizer property for BERT Embedding. ( #136 ) \ud83d\udeb8 Add predict_kwargs for models predict() function \u26a1\ufe0f Change multi-label classification's default loss function to binary_crossentropy ( #151 )","title":"0.5.1 - 2019.07.15"},{"location":"about/release-notes/#050-20190711","text":"\ud83c\udf89\ud83c\udf89 tf.keras version \ud83c\udf89\ud83c\udf89 \ud83c\udf89 Rewrite Kashgari using tf.keras ( #77 ) \ud83c\udf89 Rewrite Documents \u2728 Add TPU support \u2728 Add TF-Serving support. \u2728 Add advance customization support, like multi-input model \ud83d\udc0e Performance optimization","title":"0.5.0 - 2019.07.11"},{"location":"about/release-notes/#legacy-version-changelog","text":"","title":"Legacy Version Changelog"},{"location":"about/release-notes/#026-20190712","text":"\ud83d\udcdd Add tf.keras version info \ud83d\udc1b Fixing lstm issue in labeling model ( #125 )","title":"0.2.6 - 2019.07.12"},{"location":"about/release-notes/#024-20190606","text":"Add BERT output feature layer fine-tune support. Discussion: ( #103 ) Add BERT output feature layer number selection, default 4 according to BERT paper Fix BERT embedding token index offset issue ( #104","title":"0.2.4 - 2019.06.06"},{"location":"about/release-notes/#021-20190305","text":"fix missing sequence_labeling_tokenize_add_bos_eos config","title":"0.2.1 - 2019.03.05"},{"location":"about/release-notes/#020","text":"multi-label classification for all classification models support cuDNN cell for sequence labeling add option for output BOS and EOS in sequence labeling result, fix #31","title":"0.2.0"},{"location":"about/release-notes/#019","text":"add AVCNNModel , KMaxCNNModel , RCNNModel , AVRNNModel , DropoutBGRUModel , DropoutAVRNNModel model to classification task. fix several small bugs","title":"0.1.9"},{"location":"about/release-notes/#018","text":"fix BERT Embedding model's to_json function, issue #19","title":"0.1.8"},{"location":"about/release-notes/#017","text":"remove class candidates filter to fix #16 overwrite init function in CustomEmbedding add parameter check to custom_embedding layer add keras-bert version to setup.py file","title":"0.1.7"},{"location":"about/release-notes/#016","text":"add output_dict , debug_info params to text_classification model add output_dict , debug_info and chunk_joiner params to text_classification model fix possible crash at data_generator","title":"0.1.6"},{"location":"about/release-notes/#015","text":"fix sequence labeling evaluate result output refactor model save and load function","title":"0.1.5"},{"location":"about/release-notes/#014","text":"fix classification model evaluate result output change test settings","title":"0.1.4"},{"location":"advance-use/handle-numeric-features/","text":"Handle Numeric features # This feature is a experimental feature https://github.com/BrikerMan/Kashgari/issues/90 Some time, except the text, we have some additional features like text formatting (italic, bold, centered), position in text and more. Kashgari provides NumericFeaturesEmbedding and StackedEmbedding for this kine data. Here is the details. If you have a dataset like this. token = NLP start_of_p = True bold = True center = True B - Category token = Projects start_of_p = False bold = True center = True I - Category token = Project start_of_p = True bold = True center = False B - Project - name token = Name start_of_p = False bold = True center = False I - Project - name token = : start_of_p = False bold = False center = False I - Project - name First, numerize your additional features. Convert your data to this. Remember to leave 0 for padding. text = [ 'NLP' , 'Projects' , 'Project' , 'Name' , ':' ] start_of_p = [ 1 , 2 , 1 , 2 , 2 ] bold = [ 1 , 1 , 1 , 1 , 2 ] center = [ 1 , 1 , 2 , 2 , 2 ] label = [ 'B-Category' , 'I-Category' , 'B-Project-name' , 'I-Project-name' , 'I-Project-name' ] Then you have four input sequence and one output sequence. Prepare your embedding layers. import kashgari from kashgari.embeddings import NumericFeaturesEmbedding , BareEmbedding , StackedEmbedding import logging logging . basicConfig ( level = 'DEBUG' ) text = [ 'NLP' , 'Projects' , 'Project' , 'Name' , ':' ] start_of_p = [ 1 , 2 , 1 , 2 , 2 ] bold = [ 1 , 1 , 1 , 1 , 2 ] center = [ 1 , 1 , 2 , 2 , 2 ] label = [ 'B-Category' , 'I-Category' , 'B-ProjectName' , 'I-ProjectName' , 'I-ProjectName' ] text_list = [ text ] * 100 start_of_p_list = [ start_of_p ] * 100 bold_list = [ bold ] * 100 center_list = [ center ] * 100 label_list = [ label ] * 100 SEQUENCE_LEN = 100 # You can use WordEmbedding or BERTEmbedding for your text embedding text_embedding = BareEmbedding ( task = kashgari . LABELING , sequence_length = SEQUENCE_LEN ) start_of_p_embedding = NumericFeaturesEmbedding ( feature_count = 2 , feature_name = 'start_of_p' , sequence_length = SEQUENCE_LEN ) bold_embedding = NumericFeaturesEmbedding ( feature_count = 2 , feature_name = 'bold' , sequence_length = SEQUENCE_LEN ) center_embedding = NumericFeaturesEmbedding ( feature_count = 2 , feature_name = 'center' , sequence_length = SEQUENCE_LEN ) # first one must be the text embedding stack_embedding = StackedEmbedding ([ text_embedding , start_of_p_embedding , bold_embedding , center_embedding ]) x = ( text_list , start_of_p_list , bold_list , center_list ) y = label_list stack_embedding . analyze_corpus ( x , y ) # Now we can embed with this stacked embedding layer print ( stack_embedding . embed ( x )) Once embedding layer prepared, you could use all of the classification and labeling models. # We can build any labeling model with this embedding from kashgari.tasks.labeling import BLSTMModel model = BLSTMModel ( embedding = stack_embedding ) model . fit ( x , y ) print ( model . predict ( x )) print ( model . predict_entities ( x )) This is the struct of this model.","title":"Handle Numeric features"},{"location":"advance-use/handle-numeric-features/#handle-numeric-features","text":"This feature is a experimental feature https://github.com/BrikerMan/Kashgari/issues/90 Some time, except the text, we have some additional features like text formatting (italic, bold, centered), position in text and more. Kashgari provides NumericFeaturesEmbedding and StackedEmbedding for this kine data. Here is the details. If you have a dataset like this. token = NLP start_of_p = True bold = True center = True B - Category token = Projects start_of_p = False bold = True center = True I - Category token = Project start_of_p = True bold = True center = False B - Project - name token = Name start_of_p = False bold = True center = False I - Project - name token = : start_of_p = False bold = False center = False I - Project - name First, numerize your additional features. Convert your data to this. Remember to leave 0 for padding. text = [ 'NLP' , 'Projects' , 'Project' , 'Name' , ':' ] start_of_p = [ 1 , 2 , 1 , 2 , 2 ] bold = [ 1 , 1 , 1 , 1 , 2 ] center = [ 1 , 1 , 2 , 2 , 2 ] label = [ 'B-Category' , 'I-Category' , 'B-Project-name' , 'I-Project-name' , 'I-Project-name' ] Then you have four input sequence and one output sequence. Prepare your embedding layers. import kashgari from kashgari.embeddings import NumericFeaturesEmbedding , BareEmbedding , StackedEmbedding import logging logging . basicConfig ( level = 'DEBUG' ) text = [ 'NLP' , 'Projects' , 'Project' , 'Name' , ':' ] start_of_p = [ 1 , 2 , 1 , 2 , 2 ] bold = [ 1 , 1 , 1 , 1 , 2 ] center = [ 1 , 1 , 2 , 2 , 2 ] label = [ 'B-Category' , 'I-Category' , 'B-ProjectName' , 'I-ProjectName' , 'I-ProjectName' ] text_list = [ text ] * 100 start_of_p_list = [ start_of_p ] * 100 bold_list = [ bold ] * 100 center_list = [ center ] * 100 label_list = [ label ] * 100 SEQUENCE_LEN = 100 # You can use WordEmbedding or BERTEmbedding for your text embedding text_embedding = BareEmbedding ( task = kashgari . LABELING , sequence_length = SEQUENCE_LEN ) start_of_p_embedding = NumericFeaturesEmbedding ( feature_count = 2 , feature_name = 'start_of_p' , sequence_length = SEQUENCE_LEN ) bold_embedding = NumericFeaturesEmbedding ( feature_count = 2 , feature_name = 'bold' , sequence_length = SEQUENCE_LEN ) center_embedding = NumericFeaturesEmbedding ( feature_count = 2 , feature_name = 'center' , sequence_length = SEQUENCE_LEN ) # first one must be the text embedding stack_embedding = StackedEmbedding ([ text_embedding , start_of_p_embedding , bold_embedding , center_embedding ]) x = ( text_list , start_of_p_list , bold_list , center_list ) y = label_list stack_embedding . analyze_corpus ( x , y ) # Now we can embed with this stacked embedding layer print ( stack_embedding . embed ( x )) Once embedding layer prepared, you could use all of the classification and labeling models. # We can build any labeling model with this embedding from kashgari.tasks.labeling import BLSTMModel model = BLSTMModel ( embedding = stack_embedding ) model . fit ( x , y ) print ( model . predict ( x )) print ( model . predict_entities ( x )) This is the struct of this model.","title":"Handle Numeric features"},{"location":"advance-use/multi-output-model/","text":"Customize Multi Output Model # It is very easy to customize your own multi output model. Lets assume you have dataset like this, One input and two output. Example code at file tests/test_custom_multi_output_classification.py . x = [ [ '\u6211' , '\u60f3' , '\u5168' , '\u90e8' , '\u56de' , '\u590d' ], [ '\u6211' , '\u60f3' , '\u4e0a' , 'q' , 'q' , '\u4e86' ], [ '\u4f60' , '\u53bb' , '\u8fc7' , '\u8d4c' , '\u573a' , '\u5417' ], [ '\u662f' , '\u6211' , '\u662f' , '\u8bf4' , '\u4f60' , '\u6709' , '\u51e0' , '\u4e2a' , '\u5144' , '\u5f1f' , '\u59d0' , '\u59b9' , '\u4e0d' , '\u662f' , '\u4f60' , '\u81ea' , '\u5df1' , '\u8bf4' ], [ '\u5e7f' , '\u897f' , '\u65b0' , '\u95fb' , '\u7f51' ] ] output_1 = [ [ 0. 0. 1. ] [ 1. 0. 0. ] [ 1. 0. 0. ] [ 0. 0. 1. ] [ 1. 0. 0. ]] output_2 = [ [ 0. 1. 0. ] [ 0. 0. 1. ] [ 0. 0. 1. ] [ 1. 0. 0. ] [ 0. 0. 1. ]] Then you need to create a customized processor inhered from the ClassificationProcessor . import kashgari import numpy as np from typing import Tuple , List , Optional , Dict , Any from kashgari.processors.classification_processor import ClassificationProcessor class MultiOutputProcessor ( ClassificationProcessor ): def process_y_dataset ( self , data : Tuple [ List [ List [ str ]], ... ], maxlens : Optional [ Tuple [ int , ... ]] = None , subset : Optional [ List [ int ]] = None ) -> Tuple [ np . ndarray , ... ]: # Data already converted to one-hot # Only need to get the subset result = [] for index , dataset in enumerate ( data ): if subset is not None : target = kashgari . utils . get_list_subset ( dataset , subset ) else : target = dataset result . append ( np . array ( target )) if len ( result ) == 1 : return result [ 0 ] else : return tuple ( result ) Then build your own model inhered from the BaseClassificationModel import kashgari import tensorflow as tf from typing import Tuple , List , Optional , Dict , Any from kashgari.layers import L from kashgari.tasks.classification.base_model import BaseClassificationModel class MultiOutputModel ( BaseClassificationModel ): @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: return { 'layer_bi_lstm' : { 'units' : 256 , 'return_sequences' : False } } # Build your own model def build_model_arc ( self ): config = self . hyper_parameters embed_model = self . embedding . embed_model layer_bi_lstm = L . Bidirectional ( L . LSTM ( ** config [ 'layer_bi_lstm' ]), name = 'layer_bi_lstm' ) layer_output_1 = L . Dense ( 3 , activation = 'sigmoid' , name = 'layer_output_1' ) layer_output_2 = L . Dense ( 3 , activation = 'sigmoid' , name = 'layer_output_2' ) tensor = layer_bi_lstm ( embed_model . output ) output_tensor_1 = layer_output_1 ( tensor ) output_tensor_2 = layer_output_2 ( tensor ) self . tf_model = tf . keras . Model ( embed_model . inputs , [ output_tensor_1 , output_tensor_2 ]) # Rewrite your predict function def predict ( self , x_data , batch_size = None , debug_info = False , threshold = 0.5 ): tensor = self . embedding . process_x_dataset ( x_data ) pred = self . tf_model . predict ( tensor , batch_size = batch_size ) output_1 = pred [ 0 ] output_2 = pred [ 1 ] output_1 [ output_1 >= threshold ] = 1 output_1 [ output_1 < threshold ] = 0 output_2 [ output_2 >= threshold ] = 1 output_2 [ output_2 < threshold ] = 0 return output_1 , output_2 Tada, all done, Now build your own model with customized processor from kashgari.embeddings import BareEmbedding # Use your processor to init embedding, You can use any embedding layer provided by kashgari here processor = MultiOutputProcessor () embedding = BareEmbedding ( processor = processor ) m = MultiOutputModel ( embedding = embedding ) m . build_model ( train_x , ( output_1 , output_2 )) m . fit ( train_x , ( output_1 , output_2 ))","title":"Customize Multi Output Model"},{"location":"advance-use/multi-output-model/#customize-multi-output-model","text":"It is very easy to customize your own multi output model. Lets assume you have dataset like this, One input and two output. Example code at file tests/test_custom_multi_output_classification.py . x = [ [ '\u6211' , '\u60f3' , '\u5168' , '\u90e8' , '\u56de' , '\u590d' ], [ '\u6211' , '\u60f3' , '\u4e0a' , 'q' , 'q' , '\u4e86' ], [ '\u4f60' , '\u53bb' , '\u8fc7' , '\u8d4c' , '\u573a' , '\u5417' ], [ '\u662f' , '\u6211' , '\u662f' , '\u8bf4' , '\u4f60' , '\u6709' , '\u51e0' , '\u4e2a' , '\u5144' , '\u5f1f' , '\u59d0' , '\u59b9' , '\u4e0d' , '\u662f' , '\u4f60' , '\u81ea' , '\u5df1' , '\u8bf4' ], [ '\u5e7f' , '\u897f' , '\u65b0' , '\u95fb' , '\u7f51' ] ] output_1 = [ [ 0. 0. 1. ] [ 1. 0. 0. ] [ 1. 0. 0. ] [ 0. 0. 1. ] [ 1. 0. 0. ]] output_2 = [ [ 0. 1. 0. ] [ 0. 0. 1. ] [ 0. 0. 1. ] [ 1. 0. 0. ] [ 0. 0. 1. ]] Then you need to create a customized processor inhered from the ClassificationProcessor . import kashgari import numpy as np from typing import Tuple , List , Optional , Dict , Any from kashgari.processors.classification_processor import ClassificationProcessor class MultiOutputProcessor ( ClassificationProcessor ): def process_y_dataset ( self , data : Tuple [ List [ List [ str ]], ... ], maxlens : Optional [ Tuple [ int , ... ]] = None , subset : Optional [ List [ int ]] = None ) -> Tuple [ np . ndarray , ... ]: # Data already converted to one-hot # Only need to get the subset result = [] for index , dataset in enumerate ( data ): if subset is not None : target = kashgari . utils . get_list_subset ( dataset , subset ) else : target = dataset result . append ( np . array ( target )) if len ( result ) == 1 : return result [ 0 ] else : return tuple ( result ) Then build your own model inhered from the BaseClassificationModel import kashgari import tensorflow as tf from typing import Tuple , List , Optional , Dict , Any from kashgari.layers import L from kashgari.tasks.classification.base_model import BaseClassificationModel class MultiOutputModel ( BaseClassificationModel ): @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: return { 'layer_bi_lstm' : { 'units' : 256 , 'return_sequences' : False } } # Build your own model def build_model_arc ( self ): config = self . hyper_parameters embed_model = self . embedding . embed_model layer_bi_lstm = L . Bidirectional ( L . LSTM ( ** config [ 'layer_bi_lstm' ]), name = 'layer_bi_lstm' ) layer_output_1 = L . Dense ( 3 , activation = 'sigmoid' , name = 'layer_output_1' ) layer_output_2 = L . Dense ( 3 , activation = 'sigmoid' , name = 'layer_output_2' ) tensor = layer_bi_lstm ( embed_model . output ) output_tensor_1 = layer_output_1 ( tensor ) output_tensor_2 = layer_output_2 ( tensor ) self . tf_model = tf . keras . Model ( embed_model . inputs , [ output_tensor_1 , output_tensor_2 ]) # Rewrite your predict function def predict ( self , x_data , batch_size = None , debug_info = False , threshold = 0.5 ): tensor = self . embedding . process_x_dataset ( x_data ) pred = self . tf_model . predict ( tensor , batch_size = batch_size ) output_1 = pred [ 0 ] output_2 = pred [ 1 ] output_1 [ output_1 >= threshold ] = 1 output_1 [ output_1 < threshold ] = 0 output_2 [ output_2 >= threshold ] = 1 output_2 [ output_2 < threshold ] = 0 return output_1 , output_2 Tada, all done, Now build your own model with customized processor from kashgari.embeddings import BareEmbedding # Use your processor to init embedding, You can use any embedding layer provided by kashgari here processor = MultiOutputProcessor () embedding = BareEmbedding ( processor = processor ) m = MultiOutputModel ( embedding = embedding ) m . build_model ( train_x , ( output_1 , output_2 )) m . fit ( train_x , ( output_1 , output_2 ))","title":"Customize Multi Output Model"},{"location":"advance-use/tensorflow-serving/","text":"Tensorflow Serving # from kashgari.tasks.classification import BiGRU_Model from kashgari.corpus import SMP2018ECDTCorpus from kashgari import utils train_x , train_y = SMP2018ECDTCorpus . load_data () model = BiGRU_Model () model . fit ( train_x , train_y ) # Save model utils . convert_to_saved_model ( model , model_path = 'saved_model/bgru' , version = 1 ) Then run tensorflow-serving. docker run -t --rm -p 8501 :8501 -v \"path_to/saved_model:/models/\" -e MODEL_NAME = bgru tensorflow/serving Load processor from model, then predict with serving. import requests from kashgari import utils import numpy as np x = [ 'Hello' , 'World' ] # Pre-processor data processor = utils . load_processor ( model_path = 'saved_model/bgru/1' ) tensor = processor . process_x_dataset ([ x ]) # array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32) # if you using BERT, you need to reformat tensor first # ------ Only for BERT Embedding Start -------- tensor = [{ \"Input-Token:0\" : i . tolist (), \"Input-Segment:0\" : np . zeros ( i . shape ) . tolist () } for i in tensor ] # ------ Only for BERT Embedding End ---------- # predict r = requests . post ( \"http://localhost:8501/v1/models/bgru:predict\" , json = { \"instances\" : tensor . tolist ()}) preds = r . json ()[ 'predictions' ] # Convert result back to labels labels = processor . reverse_numerize_label_sequences ( np . array ( preds ) . argmax ( - 1 )) # labels = ['video']","title":"Tensorflow Serving"},{"location":"advance-use/tensorflow-serving/#tensorflow-serving","text":"from kashgari.tasks.classification import BiGRU_Model from kashgari.corpus import SMP2018ECDTCorpus from kashgari import utils train_x , train_y = SMP2018ECDTCorpus . load_data () model = BiGRU_Model () model . fit ( train_x , train_y ) # Save model utils . convert_to_saved_model ( model , model_path = 'saved_model/bgru' , version = 1 ) Then run tensorflow-serving. docker run -t --rm -p 8501 :8501 -v \"path_to/saved_model:/models/\" -e MODEL_NAME = bgru tensorflow/serving Load processor from model, then predict with serving. import requests from kashgari import utils import numpy as np x = [ 'Hello' , 'World' ] # Pre-processor data processor = utils . load_processor ( model_path = 'saved_model/bgru/1' ) tensor = processor . process_x_dataset ([ x ]) # array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32) # if you using BERT, you need to reformat tensor first # ------ Only for BERT Embedding Start -------- tensor = [{ \"Input-Token:0\" : i . tolist (), \"Input-Segment:0\" : np . zeros ( i . shape ) . tolist () } for i in tensor ] # ------ Only for BERT Embedding End ---------- # predict r = requests . post ( \"http://localhost:8501/v1/models/bgru:predict\" , json = { \"instances\" : tensor . tolist ()}) preds = r . json ()[ 'predictions' ] # Convert result back to labels labels = processor . reverse_numerize_label_sequences ( np . array ( preds ) . argmax ( - 1 )) # labels = ['video']","title":"Tensorflow Serving"},{"location":"api/callbacks/","text":"callbacks # class EvalCallBack # __init__ # Evaluate callback, calculate precision, recall and f1 at the end of each epoch step. def __init__ ( self , kash_model : BaseModel , valid_x , valid_y , step = 5 , batch_size = 256 ): Args : kash_model : the kashgari model to evaluate valid_x : feature data for evaluation valid_y : label data for evaluation step : evaluate step, default 5 batch_size : batch size, default 256 Methods # on_epoch_end # def on_epoch_end ( self , epoch , logs = None ):","title":"callbacks"},{"location":"api/callbacks/#callbacks","text":"","title":"callbacks"},{"location":"api/callbacks/#class-evalcallback","text":"","title":"class EvalCallBack"},{"location":"api/callbacks/#__init__","text":"Evaluate callback, calculate precision, recall and f1 at the end of each epoch step. def __init__ ( self , kash_model : BaseModel , valid_x , valid_y , step = 5 , batch_size = 256 ): Args : kash_model : the kashgari model to evaluate valid_x : feature data for evaluation valid_y : label data for evaluation step : evaluate step, default 5 batch_size : batch size, default 256","title":"__init__"},{"location":"api/callbacks/#methods","text":"","title":"Methods"},{"location":"api/callbacks/#on_epoch_end","text":"def on_epoch_end ( self , epoch , logs = None ):","title":"on_epoch_end"},{"location":"api/corpus/","text":"corpus # Kashgari provides several build-in corpus for testing. Chinese Daily Ner Corpus # Chinese Ner corpus cotains 20864 train samples, 4636 test samples and 2318 valid samples. Usage: from kashgari.corpus import ChineseDailyNerCorpus train_x , train_y = ChineseDailyNerCorpus . load_data ( 'train' ) test_x , test_y = ChineseDailyNerCorpus . load_data ( 'test' ) valid_x , valid_y = ChineseDailyNerCorpus . load_data ( 'valid' ) Data Sample: >>> x [ 0 ] [ '\u6d77' , '\u9493' , '\u6bd4' , '\u8d5b' , '\u5730' , '\u70b9' , '\u5728' , '\u53a6' , '\u95e8' ] >>> y [ 0 ] [ 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'B-LOC' , 'I-LOC' ] SMP2018 ECDT Human-Computer Dialogue Classification Corpus # https://worksheets.codalab.org/worksheets/0x27203f932f8341b79841d50ce0fd684f/ This dataset is released by the Evaluation of Chinese Human-Computer Dialogue Technology (SMP2018-ECDT) task 1 and is provided by the iFLYTEK Corporation, which is a Chinese human-computer dialogue dataset. label query 0 weather \u4eca\u5929\u4e1c\u839e\u5929\u6c14\u5982\u4f55 1 map \u4ece\u89c2\u97f3\u6865\u5230\u91cd\u5e86\u5e02\u56fe\u4e66\u9986\u600e\u4e48\u8d70 2 cookbook \u9e2d\u86cb\u600e\u4e48\u814c\uff1f 3 health \u600e\u4e48\u6cbb\u7597\u725b\u76ae\u7663 4 chat \u5520\u4ec0\u4e48 Usage: from kashgari.corpus import SMP2018ECDTCorpus train_x , train_y = SMP2018ECDTCorpus . load_data ( 'train' ) test_x , test_y = SMP2018ECDTCorpus . load_data ( 'test' ) valid_x , valid_y = SMP2018ECDTCorpus . load_data ( 'valid' ) # Change cutter to jieba, need to install jieba first train_x , train_y = SMP2018ECDTCorpus . load_data ( 'train' , cutter = 'jieba' ) test_x , test_y = SMP2018ECDTCorpus . load_data ( 'test' , cutter = 'jieba' ) valid_x , valid_y = SMP2018ECDTCorpus . load_data ( 'valid' , cutter = 'jieba' ) Data Sample: # char cutted >>> x [ 0 ] [[ '\u7ed9' , '\u5468' , '\u7389' , '\u53d1' , '\u77ed' , '\u4fe1' ]] >>> y [ 0 ] [ 'message' ] # jieba cutted >>> x [ 0 ] [[ '\u7ed9' , '\u5468\u7389' , '\u53d1\u77ed\u4fe1' ]] >>> y [ 0 ] [ 'message' ]","title":"corpus"},{"location":"api/corpus/#corpus","text":"Kashgari provides several build-in corpus for testing.","title":"corpus"},{"location":"api/corpus/#chinese-daily-ner-corpus","text":"Chinese Ner corpus cotains 20864 train samples, 4636 test samples and 2318 valid samples. Usage: from kashgari.corpus import ChineseDailyNerCorpus train_x , train_y = ChineseDailyNerCorpus . load_data ( 'train' ) test_x , test_y = ChineseDailyNerCorpus . load_data ( 'test' ) valid_x , valid_y = ChineseDailyNerCorpus . load_data ( 'valid' ) Data Sample: >>> x [ 0 ] [ '\u6d77' , '\u9493' , '\u6bd4' , '\u8d5b' , '\u5730' , '\u70b9' , '\u5728' , '\u53a6' , '\u95e8' ] >>> y [ 0 ] [ 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'B-LOC' , 'I-LOC' ]","title":"Chinese Daily Ner Corpus"},{"location":"api/corpus/#smp2018-ecdt-human-computer-dialogue-classification-corpus","text":"https://worksheets.codalab.org/worksheets/0x27203f932f8341b79841d50ce0fd684f/ This dataset is released by the Evaluation of Chinese Human-Computer Dialogue Technology (SMP2018-ECDT) task 1 and is provided by the iFLYTEK Corporation, which is a Chinese human-computer dialogue dataset. label query 0 weather \u4eca\u5929\u4e1c\u839e\u5929\u6c14\u5982\u4f55 1 map \u4ece\u89c2\u97f3\u6865\u5230\u91cd\u5e86\u5e02\u56fe\u4e66\u9986\u600e\u4e48\u8d70 2 cookbook \u9e2d\u86cb\u600e\u4e48\u814c\uff1f 3 health \u600e\u4e48\u6cbb\u7597\u725b\u76ae\u7663 4 chat \u5520\u4ec0\u4e48 Usage: from kashgari.corpus import SMP2018ECDTCorpus train_x , train_y = SMP2018ECDTCorpus . load_data ( 'train' ) test_x , test_y = SMP2018ECDTCorpus . load_data ( 'test' ) valid_x , valid_y = SMP2018ECDTCorpus . load_data ( 'valid' ) # Change cutter to jieba, need to install jieba first train_x , train_y = SMP2018ECDTCorpus . load_data ( 'train' , cutter = 'jieba' ) test_x , test_y = SMP2018ECDTCorpus . load_data ( 'test' , cutter = 'jieba' ) valid_x , valid_y = SMP2018ECDTCorpus . load_data ( 'valid' , cutter = 'jieba' ) Data Sample: # char cutted >>> x [ 0 ] [[ '\u7ed9' , '\u5468' , '\u7389' , '\u53d1' , '\u77ed' , '\u4fe1' ]] >>> y [ 0 ] [ 'message' ] # jieba cutted >>> x [ 0 ] [[ '\u7ed9' , '\u5468\u7389' , '\u53d1\u77ed\u4fe1' ]] >>> y [ 0 ] [ 'message' ]","title":"SMP2018 ECDT Human-Computer Dialogue Classification Corpus"},{"location":"api/tasks.classification/","text":"tasks.classification # All Text classification models share the same API. __init__ # def __init__ ( self , embedding : Optional [ Embedding ] = None , hyper_parameters : Optional [ Dict [ str , Dict [ str , Any ]]] = None ) Args : embedding : model embedding hyper_parameters : a dict of hyper_parameters. You could change customize hyper_parameters like this: # get default hyper_parameters hyper_parameters = BiLSTM_Model . get_default_hyper_parameters () # change lstm hidden unit to 12 hyper_parameters [ 'layer_blstm' ][ 'units' ] = 12 # init new model with customized hyper_parameters labeling_model = BiLSTM_Model ( hyper_parameters = hyper_parameters ) labeling_model . fit ( x , y ) Properties # token2idx # Returns model's token index map, type: Dict[str, int] label2idx # Returns model's label index map, type: Dict[str, int] Methods # get_default_hyper_parameters # Return the defualt hyper parameters You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: Returns : dict of the defualt hyper parameters build_model_arc # build model architectural, define models structure in this function. You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode def build_model_arc ( self ): build_model # build model with corpus def build_model ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ) Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data build_multi_gpu_model # Build multi-GPU model with corpus def build_multi_gpu_model ( self , gpus : int , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], cpu_merge : bool = True , cpu_relocation : bool = False , x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : gpus : Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data build_tpu_model # Build TPU model with corpus def build_tpu_model ( self , strategy : tf . contrib . distribute . TPUStrategy , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : strategy : TPUDistributionStrategy . The strategy to use for replicating model across multiple TPU cores. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data compile_model # Configures the model for training. Using compile() function of tf.keras.Model def compile_model ( self , ** kwargs ): Args : **kwargs : arguments passed to compile() function of tf.keras.Model Defaults : loss : categorical_crossentropy optimizer : adam metrics : ['accuracy'] get_data_generator # data generator for fit_generator def get_data_generator ( self , x_data , y_data , batch_size : int = 64 , shuffle : bool = True ) Args : x_data : Array of feature data (if the model has a single input), or tuple of feature data array (if the model has multiple inputs) y_data : Array of label data batch_size : Number of samples per gradient update, default to 64. shuffle : Returns : data generator fit # Trains the model for a given number of epochs with fit_generator (iterations on a dataset). def fit ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object. fit_without_generator # Trains the model for a given number of epochs (iterations on a dataset). Large memory Cost. def fit_without_generator ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object. predict # Generates output predictions for the input samples. Computation is done in batches. def predict ( self , x_data , batch_size = None , multi_label_threshold : float = 0.5 , debug_info = False , predict_kwargs : Dict = None ): Args : x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. multi_label_threshold : debug_info : Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : array of predictions. predict_top_k_class # Generates output predictions with confidence for the input samples. Computation is done in batches. def predict_top_k_class ( self , x_data , top_k = 5 , batch_size = 32 , debug_info = False , predict_kwargs : Dict = None ) -> List [ Dict ]: Args : x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). top_k : int batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : array(s) of prediction result dict. sample result of single-label classification: [ { \"label\" : \"chat\" , \"confidence\" : 0.5801531 , \"candidates\" : [ { \"label\" : \"cookbook\" , \"confidence\" : 0.1886314 }, { \"label\" : \"video\" , \"confidence\" : 0.13805099 }, { \"label\" : \"health\" , \"confidence\" : 0.013852648 }, { \"label\" : \"translation\" , \"confidence\" : 0.012913573 } ] } ] sample result of multi-label classification: [ { \"candidates\" : [ { \"confidence\" : 0.9959336 , \"label\" : \"toxic\" }, { \"confidence\" : 0.9358089 , \"label\" : \"obscene\" }, { \"confidence\" : 0.6882098 , \"label\" : \"insult\" }, { \"confidence\" : 0.13540423 , \"label\" : \"severe_toxic\" }, { \"confidence\" : 0.017219543 , \"label\" : \"identity_hate\" } ] } ] evaluate # Evaluate model def evaluate ( self , x_data , y_data , batch_size = None , digits = 4 , debug_info = False ) -> Tuple [ float , float , Dict ]: Args : x_data : y_data : batch_size : digits : debug_info : save # Save model info json and model weights to given folder path def save ( self , model_path : str ): Args : model_path : target model folder path info # Returns a dictionary containing the configuration of the model. def info ( self )","title":"tasks.classification"},{"location":"api/tasks.classification/#tasksclassification","text":"All Text classification models share the same API.","title":"tasks.classification"},{"location":"api/tasks.classification/#__init__","text":"def __init__ ( self , embedding : Optional [ Embedding ] = None , hyper_parameters : Optional [ Dict [ str , Dict [ str , Any ]]] = None ) Args : embedding : model embedding hyper_parameters : a dict of hyper_parameters. You could change customize hyper_parameters like this: # get default hyper_parameters hyper_parameters = BiLSTM_Model . get_default_hyper_parameters () # change lstm hidden unit to 12 hyper_parameters [ 'layer_blstm' ][ 'units' ] = 12 # init new model with customized hyper_parameters labeling_model = BiLSTM_Model ( hyper_parameters = hyper_parameters ) labeling_model . fit ( x , y )","title":"__init__"},{"location":"api/tasks.classification/#properties","text":"","title":"Properties"},{"location":"api/tasks.classification/#token2idx","text":"Returns model's token index map, type: Dict[str, int]","title":"token2idx"},{"location":"api/tasks.classification/#label2idx","text":"Returns model's label index map, type: Dict[str, int]","title":"label2idx"},{"location":"api/tasks.classification/#methods","text":"","title":"Methods"},{"location":"api/tasks.classification/#get_default_hyper_parameters","text":"Return the defualt hyper parameters You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: Returns : dict of the defualt hyper parameters","title":"get_default_hyper_parameters"},{"location":"api/tasks.classification/#build_model_arc","text":"build model architectural, define models structure in this function. You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode def build_model_arc ( self ):","title":"build_model_arc"},{"location":"api/tasks.classification/#build_model","text":"build model with corpus def build_model ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ) Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"build_model"},{"location":"api/tasks.classification/#build_multi_gpu_model","text":"Build multi-GPU model with corpus def build_multi_gpu_model ( self , gpus : int , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], cpu_merge : bool = True , cpu_relocation : bool = False , x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : gpus : Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"build_multi_gpu_model"},{"location":"api/tasks.classification/#build_tpu_model","text":"Build TPU model with corpus def build_tpu_model ( self , strategy : tf . contrib . distribute . TPUStrategy , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : strategy : TPUDistributionStrategy . The strategy to use for replicating model across multiple TPU cores. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"build_tpu_model"},{"location":"api/tasks.classification/#compile_model","text":"Configures the model for training. Using compile() function of tf.keras.Model def compile_model ( self , ** kwargs ): Args : **kwargs : arguments passed to compile() function of tf.keras.Model Defaults : loss : categorical_crossentropy optimizer : adam metrics : ['accuracy']","title":"compile_model"},{"location":"api/tasks.classification/#get_data_generator","text":"data generator for fit_generator def get_data_generator ( self , x_data , y_data , batch_size : int = 64 , shuffle : bool = True ) Args : x_data : Array of feature data (if the model has a single input), or tuple of feature data array (if the model has multiple inputs) y_data : Array of label data batch_size : Number of samples per gradient update, default to 64. shuffle : Returns : data generator","title":"get_data_generator"},{"location":"api/tasks.classification/#fit","text":"Trains the model for a given number of epochs with fit_generator (iterations on a dataset). def fit ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object.","title":"fit"},{"location":"api/tasks.classification/#fit_without_generator","text":"Trains the model for a given number of epochs (iterations on a dataset). Large memory Cost. def fit_without_generator ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object.","title":"fit_without_generator"},{"location":"api/tasks.classification/#predict","text":"Generates output predictions for the input samples. Computation is done in batches. def predict ( self , x_data , batch_size = None , multi_label_threshold : float = 0.5 , debug_info = False , predict_kwargs : Dict = None ): Args : x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. multi_label_threshold : debug_info : Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : array of predictions.","title":"predict"},{"location":"api/tasks.classification/#predict_top_k_class","text":"Generates output predictions with confidence for the input samples. Computation is done in batches. def predict_top_k_class ( self , x_data , top_k = 5 , batch_size = 32 , debug_info = False , predict_kwargs : Dict = None ) -> List [ Dict ]: Args : x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). top_k : int batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : array(s) of prediction result dict. sample result of single-label classification: [ { \"label\" : \"chat\" , \"confidence\" : 0.5801531 , \"candidates\" : [ { \"label\" : \"cookbook\" , \"confidence\" : 0.1886314 }, { \"label\" : \"video\" , \"confidence\" : 0.13805099 }, { \"label\" : \"health\" , \"confidence\" : 0.013852648 }, { \"label\" : \"translation\" , \"confidence\" : 0.012913573 } ] } ] sample result of multi-label classification: [ { \"candidates\" : [ { \"confidence\" : 0.9959336 , \"label\" : \"toxic\" }, { \"confidence\" : 0.9358089 , \"label\" : \"obscene\" }, { \"confidence\" : 0.6882098 , \"label\" : \"insult\" }, { \"confidence\" : 0.13540423 , \"label\" : \"severe_toxic\" }, { \"confidence\" : 0.017219543 , \"label\" : \"identity_hate\" } ] } ]","title":"predict_top_k_class"},{"location":"api/tasks.classification/#evaluate","text":"Evaluate model def evaluate ( self , x_data , y_data , batch_size = None , digits = 4 , debug_info = False ) -> Tuple [ float , float , Dict ]: Args : x_data : y_data : batch_size : digits : debug_info :","title":"evaluate"},{"location":"api/tasks.classification/#save","text":"Save model info json and model weights to given folder path def save ( self , model_path : str ): Args : model_path : target model folder path","title":"save"},{"location":"api/tasks.classification/#info","text":"Returns a dictionary containing the configuration of the model. def info ( self )","title":"info"},{"location":"api/tasks.labeling/","text":"tasks.labeling # All Text labeling models share the same API. __init__ # def __init__ ( self , embedding : Optional [ Embedding ] = None , hyper_parameters : Optional [ Dict [ str , Dict [ str , Any ]]] = None ) Args : embedding : model embedding hyper_parameters : a dict of hyper_parameters. You could change customize hyper_parameters like this:: # get default hyper_parameters hyper_parameters = BiLSTM_Model . get_default_hyper_parameters () # change lstm hidden unit to 12 hyper_parameters [ 'layer_blstm' ][ 'units' ] = 12 # init new model with customized hyper_parameters labeling_model = BiLSTM_Model ( hyper_parameters = hyper_parameters ) labeling_model . fit ( x , y ) Properties # token2idx # Returns model's token index map, type: Dict[str, int] label2idx # Returns model's label index map, type: Dict[str, int] Methods # get_default_hyper_parameters # Return the defualt hyper parameters You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: Returns : dict of the defualt hyper parameters build_model_arc # build model architectural, define models structure in this function. You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode def build_model_arc ( self ): build_model # build model with corpus def build_model ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ) Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data build_multi_gpu_model # Build multi-GPU model with corpus def build_multi_gpu_model ( self , gpus : int , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], cpu_merge : bool = True , cpu_relocation : bool = False , x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : gpus : Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data build_tpu_model # Build TPU model with corpus def build_tpu_model ( self , strategy : tf . contrib . distribute . TPUStrategy , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : strategy : TPUDistributionStrategy . The strategy to use for replicating model across multiple TPU cores. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data compile_model # Configures the model for training. Using compile() function of tf.keras.Model def compile_model ( self , ** kwargs ): Args : **kwargs : arguments passed to compile() function of tf.keras.Model Defaults : loss : categorical_crossentropy optimizer : adam metrics : ['accuracy'] get_data_generator # data generator for fit_generator def get_data_generator ( self , x_data , y_data , batch_size : int = 64 , shuffle : bool = True ) Args : x_data : Array of feature data (if the model has a single input), or tuple of feature data array (if the model has multiple inputs) y_data : Array of label data batch_size : Number of samples per gradient update, default to 64. shuffle : Returns : data generator fit # Trains the model for a given number of epochs with fit_generator (iterations on a dataset). def fit ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object. fit_without_generator # Trains the model for a given number of epochs (iterations on a dataset). Large memory Cost. def fit_without_generator ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object. predict # Generates output predictions for the input samples. Computation is done in batches. def predict ( self , x_data , batch_size = 32 , debug_info = False , predict_kwargs : Dict = None ): Args : x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : array of predictions. predict_entities # Gets entities from sequence. def predict_entities ( self , x_data , batch_size = None , join_chunk = ' ' , debug_info = False , predict_kwargs : Dict = None ): Args : x_data: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size: Integer. If unspecified, it will default to 32. join_chunk: str or False, debug_info: Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : list: list of entity. evaluate # Evaluate model def evaluate ( self , x_data , y_data , batch_size = None , digits = 4 , debug_info = False ) -> Tuple [ float , float , Dict ]: Args : x_data : y_data : batch_size : digits : debug_info : save # Save model info json and model weights to given folder path def save ( self , model_path : str ): Args : model_path : target model folder path info # Returns a dictionary containing the configuration of the model. def info ( self )","title":"tasks.labeling"},{"location":"api/tasks.labeling/#taskslabeling","text":"All Text labeling models share the same API.","title":"tasks.labeling"},{"location":"api/tasks.labeling/#__init__","text":"def __init__ ( self , embedding : Optional [ Embedding ] = None , hyper_parameters : Optional [ Dict [ str , Dict [ str , Any ]]] = None ) Args : embedding : model embedding hyper_parameters : a dict of hyper_parameters. You could change customize hyper_parameters like this:: # get default hyper_parameters hyper_parameters = BiLSTM_Model . get_default_hyper_parameters () # change lstm hidden unit to 12 hyper_parameters [ 'layer_blstm' ][ 'units' ] = 12 # init new model with customized hyper_parameters labeling_model = BiLSTM_Model ( hyper_parameters = hyper_parameters ) labeling_model . fit ( x , y )","title":"__init__"},{"location":"api/tasks.labeling/#properties","text":"","title":"Properties"},{"location":"api/tasks.labeling/#token2idx","text":"Returns model's token index map, type: Dict[str, int]","title":"token2idx"},{"location":"api/tasks.labeling/#label2idx","text":"Returns model's label index map, type: Dict[str, int]","title":"label2idx"},{"location":"api/tasks.labeling/#methods","text":"","title":"Methods"},{"location":"api/tasks.labeling/#get_default_hyper_parameters","text":"Return the defualt hyper parameters You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: Returns : dict of the defualt hyper parameters","title":"get_default_hyper_parameters"},{"location":"api/tasks.labeling/#build_model_arc","text":"build model architectural, define models structure in this function. You must implement this function when customizing a model When you are customizing your own model, you must implement this function. Customization example: customize-your-own-mode def build_model_arc ( self ):","title":"build_model_arc"},{"location":"api/tasks.labeling/#build_model","text":"build model with corpus def build_model ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ) Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"build_model"},{"location":"api/tasks.labeling/#build_multi_gpu_model","text":"Build multi-GPU model with corpus def build_multi_gpu_model ( self , gpus : int , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], cpu_merge : bool = True , cpu_relocation : bool = False , x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : gpus : Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"build_multi_gpu_model"},{"location":"api/tasks.labeling/#build_tpu_model","text":"Build TPU model with corpus def build_tpu_model ( self , strategy : tf . contrib . distribute . TPUStrategy , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None ): Args : strategy : TPUDistributionStrategy . The strategy to use for replicating model across multiple TPU cores. x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data","title":"build_tpu_model"},{"location":"api/tasks.labeling/#compile_model","text":"Configures the model for training. Using compile() function of tf.keras.Model def compile_model ( self , ** kwargs ): Args : **kwargs : arguments passed to compile() function of tf.keras.Model Defaults : loss : categorical_crossentropy optimizer : adam metrics : ['accuracy']","title":"compile_model"},{"location":"api/tasks.labeling/#get_data_generator","text":"data generator for fit_generator def get_data_generator ( self , x_data , y_data , batch_size : int = 64 , shuffle : bool = True ) Args : x_data : Array of feature data (if the model has a single input), or tuple of feature data array (if the model has multiple inputs) y_data : Array of label data batch_size : Number of samples per gradient update, default to 64. shuffle : Returns : data generator","title":"get_data_generator"},{"location":"api/tasks.labeling/#fit","text":"Trains the model for a given number of epochs with fit_generator (iterations on a dataset). def fit ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object.","title":"fit"},{"location":"api/tasks.labeling/#fit_without_generator","text":"Trains the model for a given number of epochs (iterations on a dataset). Large memory Cost. def fit_without_generator ( self , x_train : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]], y_train : Union [ List [ List [ str ]], List [ str ]], x_validate : Union [ Tuple [ List [ List [ str ]], ... ], List [ List [ str ]]] = None , y_validate : Union [ List [ List [ str ]], List [ str ]] = None , batch_size : int = 64 , epochs : int = 5 , callbacks : List [ keras . callbacks . Callback ] = None , fit_kwargs : Dict = None ): Args : x_train : Array of train feature data (if the model has a single input), or tuple of train feature data array (if the model has multiple inputs) y_train : Array of train label data x_validate : Array of validation feature data (if the model has a single input), or tuple of validation feature data array (if the model has multiple inputs) y_validate : Array of validation label data batch_size : Number of samples per gradient update, default to 64. epochs : Integer. Number of epochs to train the model. default 5. callbacks : fit_kwargs : additional arguments passed to fit_generator() function from tensorflow.keras.Model Returns : A tf.keras.callbacks.History object.","title":"fit_without_generator"},{"location":"api/tasks.labeling/#predict","text":"Generates output predictions for the input samples. Computation is done in batches. def predict ( self , x_data , batch_size = 32 , debug_info = False , predict_kwargs : Dict = None ): Args : x_data : The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size : Integer. If unspecified, it will default to 32. debug_info : Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : array of predictions.","title":"predict"},{"location":"api/tasks.labeling/#predict_entities","text":"Gets entities from sequence. def predict_entities ( self , x_data , batch_size = None , join_chunk = ' ' , debug_info = False , predict_kwargs : Dict = None ): Args : x_data: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs). batch_size: Integer. If unspecified, it will default to 32. join_chunk: str or False, debug_info: Bool, Should print out the logging info. predict_kwargs : Dict, arguments passed to predict() function of tensorflow.keras.Model Returns : list: list of entity.","title":"predict_entities"},{"location":"api/tasks.labeling/#evaluate","text":"Evaluate model def evaluate ( self , x_data , y_data , batch_size = None , digits = 4 , debug_info = False ) -> Tuple [ float , float , Dict ]: Args : x_data : y_data : batch_size : digits : debug_info :","title":"evaluate"},{"location":"api/tasks.labeling/#save","text":"Save model info json and model weights to given folder path def save ( self , model_path : str ): Args : model_path : target model folder path","title":"save"},{"location":"api/tasks.labeling/#info","text":"Returns a dictionary containing the configuration of the model. def info ( self )","title":"info"},{"location":"api/utils/","text":"utils # Methods # unison_shuffled_copies # def unison_shuffled_copies ( a , b ) get_list_subset # def get_list_subset ( target : List , index_list : List [ int ]) -> List custom_object_scope # def custom_object_scope () load_model # Load saved model from saved model from model.save function def load_model ( model_path : str , load_weights : bool = True ) -> BaseModel Args : model_path: model folder path load_weights: only load model structure and vocabulary when set to False, default True. Returns : load_processor # def load_processor ( model_path : str ) -> BaseProcessor Load processor from model, When we using tf-serving, we need to use model's processor to pre-process data Args : model_path: Returns : convert_to_saved_model # Export model for tensorflow serving def convert_to_saved_model ( model : BaseModel , model_path : str , version : str = None , inputs : Optional [ Dict ] = None , outputs : Optional [ Dict ] = None ): Args : model: Target model model_path: The path to which the SavedModel will be stored. version: The model version code, default timestamp inputs: dict mapping string input names to tensors. These are added to the SignatureDef as the inputs. outputs: dict mapping string output names to tensors. These are added to the SignatureDef as the outputs.","title":"utils"},{"location":"api/utils/#utils","text":"","title":"utils"},{"location":"api/utils/#methods","text":"","title":"Methods"},{"location":"api/utils/#unison_shuffled_copies","text":"def unison_shuffled_copies ( a , b )","title":"unison_shuffled_copies"},{"location":"api/utils/#get_list_subset","text":"def get_list_subset ( target : List , index_list : List [ int ]) -> List","title":"get_list_subset"},{"location":"api/utils/#custom_object_scope","text":"def custom_object_scope ()","title":"custom_object_scope"},{"location":"api/utils/#load_model","text":"Load saved model from saved model from model.save function def load_model ( model_path : str , load_weights : bool = True ) -> BaseModel Args : model_path: model folder path load_weights: only load model structure and vocabulary when set to False, default True. Returns :","title":"load_model"},{"location":"api/utils/#load_processor","text":"def load_processor ( model_path : str ) -> BaseProcessor Load processor from model, When we using tf-serving, we need to use model's processor to pre-process data Args : model_path: Returns :","title":"load_processor"},{"location":"api/utils/#convert_to_saved_model","text":"Export model for tensorflow serving def convert_to_saved_model ( model : BaseModel , model_path : str , version : str = None , inputs : Optional [ Dict ] = None , outputs : Optional [ Dict ] = None ): Args : model: Target model model_path: The path to which the SavedModel will be stored. version: The model version code, default timestamp inputs: dict mapping string input names to tensors. These are added to the SignatureDef as the inputs. outputs: dict mapping string output names to tensors. These are added to the SignatureDef as the outputs.","title":"convert_to_saved_model"},{"location":"embeddings/","text":"Language Embeddings # Kashgari provides several embeddings for language representation. Embedding layers will convert input sequence to tensor for downstream task. Availabel embeddings list: class name description BareEmbedding random init tf.keras.layers.Embedding layer for text sequence embedding WordEmbedding pre-trained Word2Vec embedding BERTEmbedding pre-trained BERT embedding GPT2Embedding pre-trained GPT-2 embedding NumericFeaturesEmbedding random init tf.keras.layers.Embedding layer for numeric feature embedding StackedEmbedding stack other embeddings for multi-input model All embedding classes inherit from the Embedding class and implement the embed() to embed your input sequence and embed_model property which you need to build you own Model. By providing the embed() function and embed_model property, Kashgari hides the the complexity of different language embedding from users, all you need to care is which language embedding you need. Quick start # Feature Extract From Pre-trained Embedding # Feature Extraction is one of the major way to use pre-trained language embedding. Kashgari provides simple API for this task. All you need to is init a embedding object then call embed function. Here is the example. All embedding shares same embed API. import kashgari from kashgari.embeddings import BERTEmbedding # need to spesify task for the downstream task, # if use embedding for feature extraction, just set `task=kashgari.CLASSIFICATION` bert = BERTEmbedding ( '<BERT_MODEL_FOLDER>' , task = kashgari . CLASSIFICATION , sequence_length = 100 ) # call for bulk embed embed_tensor = bert . embed ([[ '\u8bed' , '\u8a00' , '\u6a21' , '\u578b' ]]) # call for single embed embed_tensor = bert . embed_one ([ '\u8bed' , '\u8a00' , '\u6a21' , '\u578b' ]) print ( embed_tensor ) # array([[-0.5001117 , 0.9344998 , -0.55165815, ..., 0.49122602, # -0.2049343 , 0.25752577], # [-1.05762 , -0.43353617, 0.54398274, ..., -0.61096823, # 0.04312163, 0.03881482], # [ 0.14332692, -0.42566583, 0.68867105, ..., 0.42449307, # 0.41105768, 0.08222893], # ..., # [-0.86124015, 0.08591427, -0.34404194, ..., 0.19915134, # -0.34176797, 0.06111742], # [-0.73940575, -0.02692179, -0.5826528 , ..., 0.26934686, # -0.29708537, 0.01855129], # [-0.85489404, 0.007399 , -0.26482674, ..., 0.16851354, # -0.36805922, -0.0052386 ]], dtype=float32) Classification and Labeling # See details at classification and labeling tutorial. Customized model # You can access the tf.keras model of embedding and add your own layers or any kind customizion. Just need to access the embed_model property of the embedding object.","title":"Language Embeddings"},{"location":"embeddings/#language-embeddings","text":"Kashgari provides several embeddings for language representation. Embedding layers will convert input sequence to tensor for downstream task. Availabel embeddings list: class name description BareEmbedding random init tf.keras.layers.Embedding layer for text sequence embedding WordEmbedding pre-trained Word2Vec embedding BERTEmbedding pre-trained BERT embedding GPT2Embedding pre-trained GPT-2 embedding NumericFeaturesEmbedding random init tf.keras.layers.Embedding layer for numeric feature embedding StackedEmbedding stack other embeddings for multi-input model All embedding classes inherit from the Embedding class and implement the embed() to embed your input sequence and embed_model property which you need to build you own Model. By providing the embed() function and embed_model property, Kashgari hides the the complexity of different language embedding from users, all you need to care is which language embedding you need.","title":"Language Embeddings"},{"location":"embeddings/#quick-start","text":"","title":"Quick start"},{"location":"embeddings/#feature-extract-from-pre-trained-embedding","text":"Feature Extraction is one of the major way to use pre-trained language embedding. Kashgari provides simple API for this task. All you need to is init a embedding object then call embed function. Here is the example. All embedding shares same embed API. import kashgari from kashgari.embeddings import BERTEmbedding # need to spesify task for the downstream task, # if use embedding for feature extraction, just set `task=kashgari.CLASSIFICATION` bert = BERTEmbedding ( '<BERT_MODEL_FOLDER>' , task = kashgari . CLASSIFICATION , sequence_length = 100 ) # call for bulk embed embed_tensor = bert . embed ([[ '\u8bed' , '\u8a00' , '\u6a21' , '\u578b' ]]) # call for single embed embed_tensor = bert . embed_one ([ '\u8bed' , '\u8a00' , '\u6a21' , '\u578b' ]) print ( embed_tensor ) # array([[-0.5001117 , 0.9344998 , -0.55165815, ..., 0.49122602, # -0.2049343 , 0.25752577], # [-1.05762 , -0.43353617, 0.54398274, ..., -0.61096823, # 0.04312163, 0.03881482], # [ 0.14332692, -0.42566583, 0.68867105, ..., 0.42449307, # 0.41105768, 0.08222893], # ..., # [-0.86124015, 0.08591427, -0.34404194, ..., 0.19915134, # -0.34176797, 0.06111742], # [-0.73940575, -0.02692179, -0.5826528 , ..., 0.26934686, # -0.29708537, 0.01855129], # [-0.85489404, 0.007399 , -0.26482674, ..., 0.16851354, # -0.36805922, -0.0052386 ]], dtype=float32)","title":"Feature Extract From Pre-trained Embedding"},{"location":"embeddings/#classification-and-labeling","text":"See details at classification and labeling tutorial.","title":"Classification and Labeling"},{"location":"embeddings/#customized-model","text":"You can access the tf.keras model of embedding and add your own layers or any kind customizion. Just need to access the embed_model property of the embedding object.","title":"Customized model"},{"location":"embeddings/bare-embedding/","text":"Bare Embedding # kashgari . embeddings . BareEmbedding ( task : str = None , sequence_length : Union [ int , str ] = 'auto' , embedding_size : int = 100 , processor : Optional [ BaseProcessor ] = None ) BareEmbedding is a random init tf.keras.layers.Embedding layer for text sequence embedding, which is the defualt embedding class for kashgari models. Arguments task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. embedding_size : Dimension of the dense embedding. Here is the sample how to use embedding class. The key difference here is that must call analyze_corpus function before using the embed function. This is because the embedding layer is not pre-trained and do not contain any word-list. We need to build word-list from the corpus. import kashgari from kashgari.embeddings import BareEmbedding embedding = BareEmbedding ( task = kashgari . CLASSIFICATION , sequence_length = 100 , embedding_size = 100 ) embedding . analyze_corpus ( x_data , y_data ) embed_tensor = embedding . embed_one ([ '\u8bed' , '\u8a00' , '\u6a21' , '\u578b' ])","title":"Bare Embedding"},{"location":"embeddings/bare-embedding/#bare-embedding","text":"kashgari . embeddings . BareEmbedding ( task : str = None , sequence_length : Union [ int , str ] = 'auto' , embedding_size : int = 100 , processor : Optional [ BaseProcessor ] = None ) BareEmbedding is a random init tf.keras.layers.Embedding layer for text sequence embedding, which is the defualt embedding class for kashgari models. Arguments task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. embedding_size : Dimension of the dense embedding. Here is the sample how to use embedding class. The key difference here is that must call analyze_corpus function before using the embed function. This is because the embedding layer is not pre-trained and do not contain any word-list. We need to build word-list from the corpus. import kashgari from kashgari.embeddings import BareEmbedding embedding = BareEmbedding ( task = kashgari . CLASSIFICATION , sequence_length = 100 , embedding_size = 100 ) embedding . analyze_corpus ( x_data , y_data ) embed_tensor = embedding . embed_one ([ '\u8bed' , '\u8a00' , '\u6a21' , '\u578b' ])","title":"Bare Embedding"},{"location":"embeddings/bert-embedding/","text":"BERT Embedding # BERTEmbedding is based on keras-bert . The embeddings itself are wrapped into our simple embedding interface so that they can be used like any other embedding. BERTEmbedding support BERT variants like ERNIE , but need to load the tensorflow checkpoint . If you intrested to use ERNIE, just download tensorflow_ernie and load like BERT Embedding. Tip When using pre-trained embedding, remember to use same tokenize tool with the embedding model, this will allow to access the full power of the embedding kashgari . embeddings . BERTEmbedding ( model_folder : str , layer_nums : int = 4 , trainable : bool = False , task : str = None , sequence_length : Union [ str , int ] = 'auto' , processor : Optional [ BaseProcessor ] = None ) Arguments model_folder : path of checkpoint folder. layer_nums : number of layers whose outputs will be concatenated into a single tensor, default 4 , output the last 4 hidden layers as the thesis suggested. trainable : whether if the model is trainable, default False and set it to True for fine-tune this embedding layer during your training. task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. Example Usage - Text Classification # Let's run a text classification model with BERT. sentences = [ \"Jim Henson was a puppeteer.\" , \"This here's an example of using the BERT tokenizer.\" , \"Why did the chicken cross the road?\" ] labels = [ \"class1\" , \"class2\" , \"class1\" ] ########## Load Bert Embedding ########## import kashgari from kashgari.embeddings import BERTEmbedding bert_embedding = BERTEmbedding ( bert_model_path , task = kashgari . CLASSIFICATION , sequence_length = 128 ) tokenizer = bert_embedding . tokenizer sentences_tokenized = [] for sentence in sentences : sentence_tokenized = tokenizer . tokenize ( sentence ) sentences_tokenized . append ( sentence_tokenized ) \"\"\" The sentences will become tokenized into: [ ['[CLS]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '.', '[SEP]'], ['[CLS]', 'this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer', '.', '[SEP]'], ['[CLS]', 'why', 'did', 'the', 'chicken', 'cross', 'the', 'road', '?', '[SEP]'] ] \"\"\" # Our tokenizer already added the BOS([CLS]) and EOS([SEP]) token # so we need to disable the default add_bos_eos setting. bert_embedding . processor . add_bos_eos = False train_x , train_y = sentences_tokenized [: 2 ], labels [: 2 ] validate_x , validate_y = sentences_tokenized [ 2 :], labels [ 2 :] ########## build model ########## from kashgari.tasks.classification import CNNLSTMModel model = CNNLSTMModel ( bert_embedding ) ########## /build model ########## model . fit ( train_x , train_y , validate_x , validate_y , epochs = 3 , batch_size = 32 ) # save model model . save ( 'path/to/save/model/to' ) Use sentence pairs for input # let's assume input pair sample is \"First do it\" \"then do it right\" , Then first tokenize the sentences using bert tokenizer. Then sentence1 = [ 'First' , 'do' , 'it' ] sentence2 = [ 'then' , 'do' , 'it' , 'right' ] sample = sentence1 + [ \"[SEP]\" ] + sentence2 # Add a special separation token `[SEP]` between two sentences tokens # Generate a new token list # ['First', 'do', 'it', '[SEP]', 'then', 'do', 'it', 'right'] train_x = [ sample ] Pre-trained models # model provider Language Link info BERT official Google Multi Language link ERNIE Baidu Chinese link Unofficial Tensorflow Version Chinese BERT WWM \u54c8\u5de5\u5927\u8baf\u98de\u8054\u5408\u5b9e\u9a8c\u5ba4 Chinese link Use Tensorflow Version","title":"BERT Embedding"},{"location":"embeddings/bert-embedding/#bert-embedding","text":"BERTEmbedding is based on keras-bert . The embeddings itself are wrapped into our simple embedding interface so that they can be used like any other embedding. BERTEmbedding support BERT variants like ERNIE , but need to load the tensorflow checkpoint . If you intrested to use ERNIE, just download tensorflow_ernie and load like BERT Embedding. Tip When using pre-trained embedding, remember to use same tokenize tool with the embedding model, this will allow to access the full power of the embedding kashgari . embeddings . BERTEmbedding ( model_folder : str , layer_nums : int = 4 , trainable : bool = False , task : str = None , sequence_length : Union [ str , int ] = 'auto' , processor : Optional [ BaseProcessor ] = None ) Arguments model_folder : path of checkpoint folder. layer_nums : number of layers whose outputs will be concatenated into a single tensor, default 4 , output the last 4 hidden layers as the thesis suggested. trainable : whether if the model is trainable, default False and set it to True for fine-tune this embedding layer during your training. task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50.","title":"BERT Embedding"},{"location":"embeddings/bert-embedding/#example-usage-text-classification","text":"Let's run a text classification model with BERT. sentences = [ \"Jim Henson was a puppeteer.\" , \"This here's an example of using the BERT tokenizer.\" , \"Why did the chicken cross the road?\" ] labels = [ \"class1\" , \"class2\" , \"class1\" ] ########## Load Bert Embedding ########## import kashgari from kashgari.embeddings import BERTEmbedding bert_embedding = BERTEmbedding ( bert_model_path , task = kashgari . CLASSIFICATION , sequence_length = 128 ) tokenizer = bert_embedding . tokenizer sentences_tokenized = [] for sentence in sentences : sentence_tokenized = tokenizer . tokenize ( sentence ) sentences_tokenized . append ( sentence_tokenized ) \"\"\" The sentences will become tokenized into: [ ['[CLS]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '.', '[SEP]'], ['[CLS]', 'this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer', '.', '[SEP]'], ['[CLS]', 'why', 'did', 'the', 'chicken', 'cross', 'the', 'road', '?', '[SEP]'] ] \"\"\" # Our tokenizer already added the BOS([CLS]) and EOS([SEP]) token # so we need to disable the default add_bos_eos setting. bert_embedding . processor . add_bos_eos = False train_x , train_y = sentences_tokenized [: 2 ], labels [: 2 ] validate_x , validate_y = sentences_tokenized [ 2 :], labels [ 2 :] ########## build model ########## from kashgari.tasks.classification import CNNLSTMModel model = CNNLSTMModel ( bert_embedding ) ########## /build model ########## model . fit ( train_x , train_y , validate_x , validate_y , epochs = 3 , batch_size = 32 ) # save model model . save ( 'path/to/save/model/to' )","title":"Example Usage - Text Classification"},{"location":"embeddings/bert-embedding/#use-sentence-pairs-for-input","text":"let's assume input pair sample is \"First do it\" \"then do it right\" , Then first tokenize the sentences using bert tokenizer. Then sentence1 = [ 'First' , 'do' , 'it' ] sentence2 = [ 'then' , 'do' , 'it' , 'right' ] sample = sentence1 + [ \"[SEP]\" ] + sentence2 # Add a special separation token `[SEP]` between two sentences tokens # Generate a new token list # ['First', 'do', 'it', '[SEP]', 'then', 'do', 'it', 'right'] train_x = [ sample ]","title":"Use sentence pairs for input"},{"location":"embeddings/bert-embedding/#pre-trained-models","text":"model provider Language Link info BERT official Google Multi Language link ERNIE Baidu Chinese link Unofficial Tensorflow Version Chinese BERT WWM \u54c8\u5de5\u5927\u8baf\u98de\u8054\u5408\u5b9e\u9a8c\u5ba4 Chinese link Use Tensorflow Version","title":"Pre-trained models"},{"location":"embeddings/gpt2-embedding/","text":"GPT2 Embedding # GPT2Embedding is based on keras-gpt-2 . The embeddings itself are wrapped into our simple embedding interface so that they can be used like any other embedding. Tip When using pre-trained embedding, remember to use same tokenize tool with the embedding model, this will allow to access the full power of the embedding kashgari . embeddings . GPT2Embedding ( model_folder : str , task : str = None , sequence_length : Union [ str , int ] = 'auto' , processor : Optional [ BaseProcessor ] = None ) Arguments model_folder : path of checkpoint folder. task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50.","title":"GPT2 Embedding"},{"location":"embeddings/gpt2-embedding/#gpt2-embedding","text":"GPT2Embedding is based on keras-gpt-2 . The embeddings itself are wrapped into our simple embedding interface so that they can be used like any other embedding. Tip When using pre-trained embedding, remember to use same tokenize tool with the embedding model, this will allow to access the full power of the embedding kashgari . embeddings . GPT2Embedding ( model_folder : str , task : str = None , sequence_length : Union [ str , int ] = 'auto' , processor : Optional [ BaseProcessor ] = None ) Arguments model_folder : path of checkpoint folder. task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50.","title":"GPT2 Embedding"},{"location":"embeddings/numeric-features-embedding/","text":"Numeric Features Embedding # NumericFeaturesEmbedding is a random init tf.keras.layers.Embedding layer for numeric feature embedding. Which usally comes togather with StackedEmbedding for represent non text features. More details checkout the example: Handle Numeric features kashgari . embeddings . NumericFeaturesEmbedding ( feature_count : int , feature_name : str , sequence_length : Union [ str , int ] = 'auto' , embedding_size : int = None , processor : Optional [ BaseProcessor ] = None ) Arguments feature_count : count of the features of this embedding. feature_name : name of the feature. sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. embedding_size : Dimension of the dense embedding.","title":"Numeric Features Embedding"},{"location":"embeddings/numeric-features-embedding/#numeric-features-embedding","text":"NumericFeaturesEmbedding is a random init tf.keras.layers.Embedding layer for numeric feature embedding. Which usally comes togather with StackedEmbedding for represent non text features. More details checkout the example: Handle Numeric features kashgari . embeddings . NumericFeaturesEmbedding ( feature_count : int , feature_name : str , sequence_length : Union [ str , int ] = 'auto' , embedding_size : int = None , processor : Optional [ BaseProcessor ] = None ) Arguments feature_count : count of the features of this embedding. feature_name : name of the feature. sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50. embedding_size : Dimension of the dense embedding.","title":"Numeric Features Embedding"},{"location":"embeddings/stacked-embedding/","text":"Stacked Embedding # StackedEmbedding is a special kind of embedding class, which will able to stack other embedding layers togather for multi-input models. More details checkout the example: Handle Numeric features kashgari . embeddings . StackedEmbedding ( embeddings : List [ Embedding ], processor : Optional [ BaseProcessor ] = None ) Arguments embeddings : list of embedding object.","title":"Stacked Embedding"},{"location":"embeddings/stacked-embedding/#stacked-embedding","text":"StackedEmbedding is a special kind of embedding class, which will able to stack other embedding layers togather for multi-input models. More details checkout the example: Handle Numeric features kashgari . embeddings . StackedEmbedding ( embeddings : List [ Embedding ], processor : Optional [ BaseProcessor ] = None ) Arguments embeddings : list of embedding object.","title":"Stacked Embedding"},{"location":"embeddings/word-embedding/","text":"Word Embedding # kashgari . embeddings . WordEmbedding ( w2v_path : str , task : str = None , w2v_kwargs : Dict [ str , Any ] = None , sequence_length : Union [ Tuple [ int , ... ], str , int ] = 'auto' , processor : Optional [ BaseProcessor ] = None ) WordEmbedding is a tf.keras.layers.Embedding layer with pre-trained Word2Vec/GloVe Emedding weights. When using pre-trained embedding, remember to use same tokenize tool with the embedding model, this will allow to access the full power of the embedding Arguments w2v_path : Word2Vec file path. task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . w2v_kwargs : params pass to the load_word2vec_format() function of gensim.models.KeyedVectors - https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50.","title":"Word Embedding"},{"location":"embeddings/word-embedding/#word-embedding","text":"kashgari . embeddings . WordEmbedding ( w2v_path : str , task : str = None , w2v_kwargs : Dict [ str , Any ] = None , sequence_length : Union [ Tuple [ int , ... ], str , int ] = 'auto' , processor : Optional [ BaseProcessor ] = None ) WordEmbedding is a tf.keras.layers.Embedding layer with pre-trained Word2Vec/GloVe Emedding weights. When using pre-trained embedding, remember to use same tokenize tool with the embedding model, this will allow to access the full power of the embedding Arguments w2v_path : Word2Vec file path. task : kashgari.CLASSIFICATION kashgari.LABELING . Downstream task type, If you only need to feature extraction, just set it as kashgari.CLASSIFICATION . w2v_kwargs : params pass to the load_word2vec_format() function of gensim.models.KeyedVectors - https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors sequence_length : 'auto' , 'variable' or integer. When using 'auto' , use the 95% of corpus length as sequence length. When using 'variable' , model input shape will set to None, which can handle various length of input, it will use the length of max sequence in every batch for sequence length. If using an integer, let's say 50 , the input output sequence length will set to 50.","title":"Word Embedding"},{"location":"tutorial/text-classification/","text":"\u6587\u672c\u5206\u7c7b # Kashgari \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\u3002\u6240\u6709\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\u90fd\u7ee7\u627f\u81ea BaseClassificationModel \u7c7b\uff0c\u63d0\u4f9b\u4e86\u540c\u6837\u7684 API\u3002\u6240\u4ee5\u5207\u6362\u6a21\u578b\u505a\u5b9e\u9a8c\u975e\u5e38\u7684\u65b9\u4fbf\u3002 \u63a5\u53e3\u6587\u6863\u8bf7\u770b\uff1a \u5206\u7c7b\u6a21\u578b API \u6587\u6863 \u5185\u7f6e\u6a21\u578b\u5217\u8868 # \u6a21\u578b\u540d\u79f0 \u6a21\u578b\u63cf\u8ff0 BiLSTM_Model BiGRU_Model CNN_Model CNN_LSTM_Model CNN_GRU_Model AVCNN_Model KMax_CNN_Model R_CNN_Model AVRNN_Model Dropout_BiGRU_Model Dropout_AVRNN_Model DPCNN_Model \u8bad\u7ec3\u5206\u7c7b\u6a21\u578b # Kashgari \u5185\u7f6e\u4e86\u4e00\u4e2a\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6\u7528\u4e8e\u6d4b\u8bd5\u3002\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\uff0c\u53ea\u9700\u8981\u628a\u6570\u636e\u96c6\u683c\u5f0f\u5316\u4e3a\u540c\u6837\u7684\u683c\u5f0f\u5373\u53ef\u3002 from kashgari.corpus import SMP2018ECDTCorpus # \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6 train_x , train_y = SMP2018ECDTCorpus . load_data ( 'train' ) valid_x , valid_y = SMP2018ECDTCorpus . load_data ( 'valid' ) test_x , test_y = SMP2018ECDTCorpus . load_data ( 'test' ) # \u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\u96c6 train_x = [[ 'Hello' , 'world' ], [ 'Hello' , 'Kashgari' ]] train_y = [ 'a' , 'b' ] valid_x , valid_y = train_x , train_y test_x , test_x = train_x , train_y \u4f7f\u7528\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002\u6240\u6709\u7684\u6a21\u578b\u90fd\u63d0\u4f9b\u540c\u6837\u7684\u63a5\u53e3\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5 BiLSTM_Model \u6a21\u578b\u66ff\u6362\u4e3a\u4efb\u4f55\u4e00\u4e2a\u5185\u7f6e\u7684\u5206\u7c7b\u6a21\u578b\u3002 import kashgari from kashgari.tasks.classification import BiLSTM_Model import logging logging . basicConfig ( level = 'DEBUG' ) model = BiLSTM_Model () model . fit ( train_x , train_y , valid_x , valid_y ) # \u9a8c\u8bc1\u6a21\u578b\uff0c\u6b64\u65b9\u6cd5\u5c06\u6253\u5370\u51fa\u8be6\u7ec6\u7684\u9a8c\u8bc1\u62a5\u544a model . evaluate ( test_x , test_y ) # \u4fdd\u5b58\u6a21\u578b\u5230 `saved_ner_model` \u76ee\u5f55\u4e0b model . save ( 'saved_classification_model' ) # \u52a0\u8f7d\u4fdd\u5b58\u6a21\u578b loaded_model = kashgari . utils . load_model ( 'saved_classification_model' ) # \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b loaded_model . predict ( test_x [: 10 ]) \u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60 # Kashgari \u5185\u7f6e\u4e86\u51e0\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6a21\u5757\uff0c\u7b80\u5316\u8fc1\u79fb\u5b66\u4e60\u6d41\u7a0b\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528 BERT \u7684\u4f8b\u5b50\u3002 import kashgari from kashgari.tasks.classification import BiGRU_Model from kashgari.embeddings import BERTEmbedding import logging logging . basicConfig ( level = 'DEBUG' ) bert_embed = BERTEmbedding ( '<PRE_TRAINED_BERT_MODEL_FOLDER>' , task = kashgari . LABELING , sequence_length = 100 ) model = BiGRU_Model ( bert_embed ) model . fit ( train_x , train_y , valid_x , valid_y ) \u4f60\u8fd8\u53ef\u4ee5\u628a BERT \u66ff\u6362\u6210 WordEmbedding \u6216\u8005 GPT2Embedding \u7b49\uff0c\u66f4\u591a\u8bf7\u67e5\u9605 Embedding \u6587\u6863 \u8c03\u6574\u6a21\u578b\u8d85\u53c2\u6570 # \u901a\u8fc7\u6a21\u578b\u7684 get_default_hyper_parameters() \u65b9\u6cd5\u53ef\u4ee5\u83b7\u53d6\u9ed8\u8ba4\u8d85\u53c2\uff0c\u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u3002\u901a\u8fc7\u4fee\u6539\u5b57\u5178\u6765\u4fee\u6539\u8d85\u53c2\u5217\u8868\u3002\u518d\u4f7f\u7528\u65b0\u7684\u8d85\u53c2\u5b57\u5178\u521d\u59cb\u5316\u6a21\u578b\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u628a layer_bi_lstm \u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u8c03\u6574\u4e3a 32\uff1a from kashgari.tasks.classification import BiLSTM_Model hyper = BiLSTM_Model . get_default_hyper_parameters () print ( hyper ) # {'layer_bi_lstm': {'units': 128, 'return_sequences': False}, 'layer_dense': {'activation': 'softmax'}} hyper [ 'layer_bi_lstm' ][ 'units' ] = 32 model = BiLSTM_Model ( hyper_parameters = hyper ) \u4f7f\u7528\u8bad\u7ec3\u56de\u8c03 # Kashgari \u662f\u57fa\u4e8e tf.keras, \u6240\u4ee5\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u5168\u90e8\u7684 tf.keras \u56de\u8c03\u7c7b \uff0c\u4f8b\u5982\u6211\u4eec\u4f7f\u7528 TensorBoard \u53ef\u89c6\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002 from tensorflow.python import keras from kashgari.tasks.classification import BiGRU_Model from kashgari.callbacks import EvalCallBack import logging logging . basicConfig ( level = 'DEBUG' ) model = BiGRU_Model () tf_board_callback = keras . callbacks . TensorBoard ( log_dir = './logs' , update_freq = 1000 ) # \u8fd9\u662f Kashgari \u5185\u7f6e\u56de\u8c03\u51fd\u6570\uff0c\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u8ba1\u7b97\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\u548c F1 eval_callback = EvalCallBack ( kash_model = model , valid_x = valid_x , valid_y = valid_y , step = 5 ) model . fit ( train_x , train_y , valid_x , valid_y , batch_size = 100 , callbacks = [ eval_callback , tf_board_callback ]) \u591a\u6807\u7b7e\u5206\u7c7b # Kashgari \u652f\u6301\u591a\u5206\u7c7b\u591a\u6807\u7b7e\u5206\u7c7b\u3002 \u5047\u8bbe\u6211\u4eec\u7684\u6570\u636e\u96c6\u662f\u8fd9\u6837\u7684\uff1a x = [ [ 'This' , 'news' , are ',very' , 'well' , 'organized' ], [ 'What' , 'extremely' , 'usefull' , 'tv' , 'show' ], [ 'The' , 'tv' , 'presenter' , 'were' , 'very' , 'well' , 'dress' ], [ 'Multi-class' , 'classification' , 'means' , 'a' , 'classification' , 'task' , 'with' , 'more' , 'than' , 'two' , 'classes' ] ] y = [ [ 'A' , 'B' ], [ 'A' ,], [ 'B' , 'C' ], [] ] \u73b0\u5728\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u4e00\u4e2a Processor \u548c Embedding \u5bf9\u8c61\uff0c\u7136\u540e\u518d\u521d\u59cb\u5316\u6211\u4eec\u7684\u6a21\u578b\u3002 from kashgari.tasks.classification import BiLSTM_Model from kashgari.processors import ClassificationProcessor from kashgari.embeddings import BareEmbedding import logging logging . basicConfig ( level = 'DEBUG' ) # \u9700\u8981\u6307\u5b9a\u6211\u4eec\u4f7f\u7528\u5206\u7c7b\u6570\u636e\u5904\u7406\u5668\uff0c\u4e14\u652f\u6301\u591a\u5206\u7c7b processor = ClassificationProcessor ( multi_label = True ) embed = BareEmbedding ( processor = processor ) model = BiLSTM_Model ( embed ) model . fit ( x , y ) \u81ea\u5b9a\u4e49\u6a21\u578b\u7ed3\u6784 # \u9664\u4e86\u5185\u7f6e\u6a21\u578b\u4ee5\u5916\uff0c\u8fd8\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u81ea\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u578b\u7ed3\u6784\u3002\u53ea\u9700\u8981\u7ee7\u627f BaseClassificationModel \u5bf9\u8c61\uff0c\u7136\u540e\u5b9e\u73b0 get_default_hyper_parameters() \u65b9\u6cd5 \u548c build_model_arc() \u65b9\u6cd5\u3002 from typing import Dict , Any from tensorflow import keras from kashgari.tasks.classification.base_model import BaseClassificationModel from kashgari.layers import L import logging logging . basicConfig ( level = 'DEBUG' ) class DoubleBLSTMModel ( BaseClassificationModel ): @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Get hyper parameters of model Returns: hyper parameters dict \"\"\" return { 'layer_blstm1' : { 'units' : 128 , 'return_sequences' : True }, 'layer_blstm2' : { 'units' : 128 , 'return_sequences' : False }, 'layer_dropout' : { 'rate' : 0.4 }, 'layer_time_distributed' : {}, 'layer_activation' : { 'activation' : 'softmax' } } def build_model_arc ( self ): \"\"\" build model architectural \"\"\" # \u6b64\u5904\u4f5c\u7528\u662f\u4ece\u4e0a\u5c42\u62ff\u5230\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u548c Embedding \u5c42\u7684\u8f93\u51fa output_dim = len ( self . pre_processor . label2idx ) config = self . hyper_parameters embed_model = self . embedding . embed_model # \u5b9a\u4e49\u4f60\u81ea\u5df1\u7684\u5c42 layer_blstm1 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm1' ]), name = 'layer_blstm1' ) layer_blstm2 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm2' ]), name = 'layer_blstm2' ) layer_dropout = L . Dropout ( ** config [ 'layer_dropout' ], name = 'layer_dropout' ) layer_time_distributed = L . TimeDistributed ( L . Dense ( output_dim , ** config [ 'layer_time_distributed' ]), name = 'layer_time_distributed' ) layer_activation = L . Activation ( ** config [ 'layer_activation' ]) # \u5b9a\u4e49\u6570\u636e\u6d41 tensor = layer_blstm1 ( embed_model . output ) tensor = layer_blstm2 ( tensor ) tensor = layer_dropout ( tensor ) tensor = layer_time_distributed ( tensor ) output_tensor = layer_activation ( tensor ) # \u521d\u59cb\u5316\u6a21\u578b self . tf_model = keras . Model ( embed_model . inputs , output_tensor ) # \u6b64\u6a21\u578b\u53ef\u4ee5\u548c\u4efb\u4f55\u4e00\u4e2a Embedding \u7ec4\u5408\u4f7f\u7528 model = DoubleBLSTMModel () model . fit ( train_x , train_y , valid_x , valid_y ) \u4f7f\u7528 CuDNN \u52a0\u901f GPU \u8bad\u7ec3 # Kashgari \u53ef\u4ee5\u4f7f\u7528 CuDNN \u5c42 \u6765\u52a0\u901f\u8bad\u7ec3\u3002CuDNNLSTM \u548c CuDNNGRU \u8bad\u7ec3\u901f\u5ea6\u6bd4 LSTM \u548c GRU \u5feb\u5f88\u591a\uff0c\u4f46\u662f\u53ea\u80fd\u5728 GPU \u4e0a\u4f7f\u7528\u3002\u5982\u679c\u9700\u8981 GPU \u8bad\u7ec3\uff0cCPU \u63a8\u65ad\uff0c\u90a3\u4e48\u4e0d\u80fd\u4f7f\u7528 CuDNN \u6765\u52a0\u901f\u8bad\u7ec3\u3002\u8bbe\u7f6e CuDNN \u65b9\u6cd5\u5982\u4e0b\uff1a kashgari . config . use_cudnn_cell = True","title":"\u6587\u672c\u5206\u7c7b"},{"location":"tutorial/text-classification/#_1","text":"Kashgari \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\u3002\u6240\u6709\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\u90fd\u7ee7\u627f\u81ea BaseClassificationModel \u7c7b\uff0c\u63d0\u4f9b\u4e86\u540c\u6837\u7684 API\u3002\u6240\u4ee5\u5207\u6362\u6a21\u578b\u505a\u5b9e\u9a8c\u975e\u5e38\u7684\u65b9\u4fbf\u3002 \u63a5\u53e3\u6587\u6863\u8bf7\u770b\uff1a \u5206\u7c7b\u6a21\u578b API \u6587\u6863","title":"\u6587\u672c\u5206\u7c7b"},{"location":"tutorial/text-classification/#_2","text":"\u6a21\u578b\u540d\u79f0 \u6a21\u578b\u63cf\u8ff0 BiLSTM_Model BiGRU_Model CNN_Model CNN_LSTM_Model CNN_GRU_Model AVCNN_Model KMax_CNN_Model R_CNN_Model AVRNN_Model Dropout_BiGRU_Model Dropout_AVRNN_Model DPCNN_Model","title":"\u5185\u7f6e\u6a21\u578b\u5217\u8868"},{"location":"tutorial/text-classification/#_3","text":"Kashgari \u5185\u7f6e\u4e86\u4e00\u4e2a\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6\u7528\u4e8e\u6d4b\u8bd5\u3002\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\uff0c\u53ea\u9700\u8981\u628a\u6570\u636e\u96c6\u683c\u5f0f\u5316\u4e3a\u540c\u6837\u7684\u683c\u5f0f\u5373\u53ef\u3002 from kashgari.corpus import SMP2018ECDTCorpus # \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6 train_x , train_y = SMP2018ECDTCorpus . load_data ( 'train' ) valid_x , valid_y = SMP2018ECDTCorpus . load_data ( 'valid' ) test_x , test_y = SMP2018ECDTCorpus . load_data ( 'test' ) # \u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\u96c6 train_x = [[ 'Hello' , 'world' ], [ 'Hello' , 'Kashgari' ]] train_y = [ 'a' , 'b' ] valid_x , valid_y = train_x , train_y test_x , test_x = train_x , train_y \u4f7f\u7528\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002\u6240\u6709\u7684\u6a21\u578b\u90fd\u63d0\u4f9b\u540c\u6837\u7684\u63a5\u53e3\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5 BiLSTM_Model \u6a21\u578b\u66ff\u6362\u4e3a\u4efb\u4f55\u4e00\u4e2a\u5185\u7f6e\u7684\u5206\u7c7b\u6a21\u578b\u3002 import kashgari from kashgari.tasks.classification import BiLSTM_Model import logging logging . basicConfig ( level = 'DEBUG' ) model = BiLSTM_Model () model . fit ( train_x , train_y , valid_x , valid_y ) # \u9a8c\u8bc1\u6a21\u578b\uff0c\u6b64\u65b9\u6cd5\u5c06\u6253\u5370\u51fa\u8be6\u7ec6\u7684\u9a8c\u8bc1\u62a5\u544a model . evaluate ( test_x , test_y ) # \u4fdd\u5b58\u6a21\u578b\u5230 `saved_ner_model` \u76ee\u5f55\u4e0b model . save ( 'saved_classification_model' ) # \u52a0\u8f7d\u4fdd\u5b58\u6a21\u578b loaded_model = kashgari . utils . load_model ( 'saved_classification_model' ) # \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b loaded_model . predict ( test_x [: 10 ])","title":"\u8bad\u7ec3\u5206\u7c7b\u6a21\u578b"},{"location":"tutorial/text-classification/#_4","text":"Kashgari \u5185\u7f6e\u4e86\u51e0\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6a21\u5757\uff0c\u7b80\u5316\u8fc1\u79fb\u5b66\u4e60\u6d41\u7a0b\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528 BERT \u7684\u4f8b\u5b50\u3002 import kashgari from kashgari.tasks.classification import BiGRU_Model from kashgari.embeddings import BERTEmbedding import logging logging . basicConfig ( level = 'DEBUG' ) bert_embed = BERTEmbedding ( '<PRE_TRAINED_BERT_MODEL_FOLDER>' , task = kashgari . LABELING , sequence_length = 100 ) model = BiGRU_Model ( bert_embed ) model . fit ( train_x , train_y , valid_x , valid_y ) \u4f60\u8fd8\u53ef\u4ee5\u628a BERT \u66ff\u6362\u6210 WordEmbedding \u6216\u8005 GPT2Embedding \u7b49\uff0c\u66f4\u591a\u8bf7\u67e5\u9605 Embedding \u6587\u6863","title":"\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60"},{"location":"tutorial/text-classification/#_5","text":"\u901a\u8fc7\u6a21\u578b\u7684 get_default_hyper_parameters() \u65b9\u6cd5\u53ef\u4ee5\u83b7\u53d6\u9ed8\u8ba4\u8d85\u53c2\uff0c\u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u3002\u901a\u8fc7\u4fee\u6539\u5b57\u5178\u6765\u4fee\u6539\u8d85\u53c2\u5217\u8868\u3002\u518d\u4f7f\u7528\u65b0\u7684\u8d85\u53c2\u5b57\u5178\u521d\u59cb\u5316\u6a21\u578b\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u628a layer_bi_lstm \u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u8c03\u6574\u4e3a 32\uff1a from kashgari.tasks.classification import BiLSTM_Model hyper = BiLSTM_Model . get_default_hyper_parameters () print ( hyper ) # {'layer_bi_lstm': {'units': 128, 'return_sequences': False}, 'layer_dense': {'activation': 'softmax'}} hyper [ 'layer_bi_lstm' ][ 'units' ] = 32 model = BiLSTM_Model ( hyper_parameters = hyper )","title":"\u8c03\u6574\u6a21\u578b\u8d85\u53c2\u6570"},{"location":"tutorial/text-classification/#_6","text":"Kashgari \u662f\u57fa\u4e8e tf.keras, \u6240\u4ee5\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u5168\u90e8\u7684 tf.keras \u56de\u8c03\u7c7b \uff0c\u4f8b\u5982\u6211\u4eec\u4f7f\u7528 TensorBoard \u53ef\u89c6\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002 from tensorflow.python import keras from kashgari.tasks.classification import BiGRU_Model from kashgari.callbacks import EvalCallBack import logging logging . basicConfig ( level = 'DEBUG' ) model = BiGRU_Model () tf_board_callback = keras . callbacks . TensorBoard ( log_dir = './logs' , update_freq = 1000 ) # \u8fd9\u662f Kashgari \u5185\u7f6e\u56de\u8c03\u51fd\u6570\uff0c\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u8ba1\u7b97\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\u548c F1 eval_callback = EvalCallBack ( kash_model = model , valid_x = valid_x , valid_y = valid_y , step = 5 ) model . fit ( train_x , train_y , valid_x , valid_y , batch_size = 100 , callbacks = [ eval_callback , tf_board_callback ])","title":"\u4f7f\u7528\u8bad\u7ec3\u56de\u8c03"},{"location":"tutorial/text-classification/#_7","text":"Kashgari \u652f\u6301\u591a\u5206\u7c7b\u591a\u6807\u7b7e\u5206\u7c7b\u3002 \u5047\u8bbe\u6211\u4eec\u7684\u6570\u636e\u96c6\u662f\u8fd9\u6837\u7684\uff1a x = [ [ 'This' , 'news' , are ',very' , 'well' , 'organized' ], [ 'What' , 'extremely' , 'usefull' , 'tv' , 'show' ], [ 'The' , 'tv' , 'presenter' , 'were' , 'very' , 'well' , 'dress' ], [ 'Multi-class' , 'classification' , 'means' , 'a' , 'classification' , 'task' , 'with' , 'more' , 'than' , 'two' , 'classes' ] ] y = [ [ 'A' , 'B' ], [ 'A' ,], [ 'B' , 'C' ], [] ] \u73b0\u5728\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u4e00\u4e2a Processor \u548c Embedding \u5bf9\u8c61\uff0c\u7136\u540e\u518d\u521d\u59cb\u5316\u6211\u4eec\u7684\u6a21\u578b\u3002 from kashgari.tasks.classification import BiLSTM_Model from kashgari.processors import ClassificationProcessor from kashgari.embeddings import BareEmbedding import logging logging . basicConfig ( level = 'DEBUG' ) # \u9700\u8981\u6307\u5b9a\u6211\u4eec\u4f7f\u7528\u5206\u7c7b\u6570\u636e\u5904\u7406\u5668\uff0c\u4e14\u652f\u6301\u591a\u5206\u7c7b processor = ClassificationProcessor ( multi_label = True ) embed = BareEmbedding ( processor = processor ) model = BiLSTM_Model ( embed ) model . fit ( x , y )","title":"\u591a\u6807\u7b7e\u5206\u7c7b"},{"location":"tutorial/text-classification/#_8","text":"\u9664\u4e86\u5185\u7f6e\u6a21\u578b\u4ee5\u5916\uff0c\u8fd8\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u81ea\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u578b\u7ed3\u6784\u3002\u53ea\u9700\u8981\u7ee7\u627f BaseClassificationModel \u5bf9\u8c61\uff0c\u7136\u540e\u5b9e\u73b0 get_default_hyper_parameters() \u65b9\u6cd5 \u548c build_model_arc() \u65b9\u6cd5\u3002 from typing import Dict , Any from tensorflow import keras from kashgari.tasks.classification.base_model import BaseClassificationModel from kashgari.layers import L import logging logging . basicConfig ( level = 'DEBUG' ) class DoubleBLSTMModel ( BaseClassificationModel ): @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Get hyper parameters of model Returns: hyper parameters dict \"\"\" return { 'layer_blstm1' : { 'units' : 128 , 'return_sequences' : True }, 'layer_blstm2' : { 'units' : 128 , 'return_sequences' : False }, 'layer_dropout' : { 'rate' : 0.4 }, 'layer_time_distributed' : {}, 'layer_activation' : { 'activation' : 'softmax' } } def build_model_arc ( self ): \"\"\" build model architectural \"\"\" # \u6b64\u5904\u4f5c\u7528\u662f\u4ece\u4e0a\u5c42\u62ff\u5230\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u548c Embedding \u5c42\u7684\u8f93\u51fa output_dim = len ( self . pre_processor . label2idx ) config = self . hyper_parameters embed_model = self . embedding . embed_model # \u5b9a\u4e49\u4f60\u81ea\u5df1\u7684\u5c42 layer_blstm1 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm1' ]), name = 'layer_blstm1' ) layer_blstm2 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm2' ]), name = 'layer_blstm2' ) layer_dropout = L . Dropout ( ** config [ 'layer_dropout' ], name = 'layer_dropout' ) layer_time_distributed = L . TimeDistributed ( L . Dense ( output_dim , ** config [ 'layer_time_distributed' ]), name = 'layer_time_distributed' ) layer_activation = L . Activation ( ** config [ 'layer_activation' ]) # \u5b9a\u4e49\u6570\u636e\u6d41 tensor = layer_blstm1 ( embed_model . output ) tensor = layer_blstm2 ( tensor ) tensor = layer_dropout ( tensor ) tensor = layer_time_distributed ( tensor ) output_tensor = layer_activation ( tensor ) # \u521d\u59cb\u5316\u6a21\u578b self . tf_model = keras . Model ( embed_model . inputs , output_tensor ) # \u6b64\u6a21\u578b\u53ef\u4ee5\u548c\u4efb\u4f55\u4e00\u4e2a Embedding \u7ec4\u5408\u4f7f\u7528 model = DoubleBLSTMModel () model . fit ( train_x , train_y , valid_x , valid_y )","title":"\u81ea\u5b9a\u4e49\u6a21\u578b\u7ed3\u6784"},{"location":"tutorial/text-classification/#cudnn-gpu","text":"Kashgari \u53ef\u4ee5\u4f7f\u7528 CuDNN \u5c42 \u6765\u52a0\u901f\u8bad\u7ec3\u3002CuDNNLSTM \u548c CuDNNGRU \u8bad\u7ec3\u901f\u5ea6\u6bd4 LSTM \u548c GRU \u5feb\u5f88\u591a\uff0c\u4f46\u662f\u53ea\u80fd\u5728 GPU \u4e0a\u4f7f\u7528\u3002\u5982\u679c\u9700\u8981 GPU \u8bad\u7ec3\uff0cCPU \u63a8\u65ad\uff0c\u90a3\u4e48\u4e0d\u80fd\u4f7f\u7528 CuDNN \u6765\u52a0\u901f\u8bad\u7ec3\u3002\u8bbe\u7f6e CuDNN \u65b9\u6cd5\u5982\u4e0b\uff1a kashgari . config . use_cudnn_cell = True","title":"\u4f7f\u7528 CuDNN \u52a0\u901f GPU \u8bad\u7ec3"},{"location":"tutorial/text-labeling/","text":"\u6587\u672c\u6807\u6ce8 # Kashgari \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\u3002\u6240\u6709\u7684\u6587\u672c\u6807\u6ce8\u6a21\u578b\u90fd\u7ee7\u627f\u81ea BaseLabelingModel \u7c7b\uff0c\u63d0\u4f9b\u4e86\u540c\u6837\u7684 API\u3002\u6240\u4ee5\u5207\u6362\u6a21\u578b\u505a\u5b9e\u9a8c\u975e\u5e38\u7684\u65b9\u4fbf\u3002 Available Models # Name Info CNN_LSTM_Model BiLSTM_Model BiLSTM_CRF_Model BiGRU_Model BiGRU_CRF_Model \u8bad\u7ec3\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b # Kashgari \u5185\u7f6e\u4e86\u4eba\u6c11\u65e5\u62a5\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c CONLL 2003 \u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u65b9\u4fbf\u5feb\u901f\u5b9e\u9a8c\u3002 \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6 # \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6 ## \u4e2d\u6587\u6570\u636e\u96c6 from kashgari.corpus import ChineseDailyNerCorpus train_x , train_y = ChineseDailyNerCorpus . load_data ( 'train' ) valid_x , valid_y = ChineseDailyNerCorpus . load_data ( 'valid' ) test_x , test_y = ChineseDailyNerCorpus . load_data ( 'test' ) ## \u82f1\u6587\u6570\u636e\u96c6 from kashgari.corpus import CONLL2003ENCorpus train_x , train_y = CONLL2003ENCorpus . load_data ( 'train' ) valid_x , valid_y = CONLL2003ENCorpus . load_data ( 'valid' ) test_x , test_y = CONLL2003ENCorpus . load_data ( 'test' ) # \u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\u96c6 train_x = [[ 'Hello' , 'world' ], [ 'Hello' , 'Kashgari' ], [ 'I' , 'love' , 'Beijing' ]] train_y = [[ 'O' , 'O' ], [ 'O' , 'B-PER' ], [ 'O' , 'B-LOC' ]] valid_x , valid_y = train_x , train_y test_x , test_x = train_x , train_y \u9664\u4e86\u4f7f\u7528\u5185\u7f6e\u6570\u636e\u96c6\uff0c\u4f60\u4e5f\u53ef\u4ee5\u52a0\u8f7d\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u6570\u636e\u683c\u5f0f\u548c\u5185\u7f6e\u6570\u636e\u96c6\u4e00\u6837\u5373\u53ef\uff0c\u5efa\u8bae\u6309\u7167 BIO \u89c4\u8303\u8fdb\u884c\u6807\u6ce8\u3002\u5185\u7f6e\u6570\u636e\u96c6\u683c\u5f0f\u5982\u4e0b\uff1a >>> print ( train_x [ 0 ]) [ '\u6d77' , '\u9493' , '\u6bd4' , '\u8d5b' , '\u5730' , '\u70b9' , '\u5728' , '\u53a6' , '\u95e8' , '\u4e0e' , '\u91d1' , '\u95e8' , '\u4e4b' , '\u95f4' , '\u7684' , '\u6d77' , '\u57df' , '\u3002' ] >>> print ( train_y [ 0 ]) [ 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'B-LOC' , 'I-LOC' , 'O' , 'B-LOC' , 'I-LOC' , 'O' , 'O' , 'O' , 'O' , 'O' , 'O' ] \u6570\u636e\u51c6\u5907\u597d\u4e86\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u4e86\u3002\u6240\u6709\u7684\u6807\u6ce8\u6a21\u578b\u63d0\u4f9b\u76f8\u540c\u7684 API\uff0c\u6240\u4ee5\u66ff\u6362\u6a21\u578b\u67b6\u6784\u975e\u5e38\u65b9\u4fbf\u3002 import kashgari from kashgari.tasks.labeling import BLSTMModel model = BLSTMModel () model . fit ( train_x , train_y , valid_x , valid_y ) # \u9a8c\u8bc1\u6a21\u578b\uff0c\u6b64\u65b9\u6cd5\u5c06\u6253\u5370\u51fa\u8be6\u7ec6\u7684\u9a8c\u8bc1\u62a5\u544a model . evaluate ( test_x , test_y ) # \u4fdd\u5b58\u6a21\u578b\u5230 `saved_ner_model` \u76ee\u5f55\u4e0b model . save ( 'saved_ner_model' ) # \u52a0\u8f7d\u4fdd\u5b58\u6a21\u578b loaded_model = kashgari . utils . load_model ( 'saved_ner_model' ) # \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b loaded_model . predict ( test_x [: 10 ]) \u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60 # Kashgari \u5185\u7f6e\u4e86\u51e0\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6a21\u5757\uff0c\u7b80\u5316\u8fc1\u79fb\u5b66\u4e60\u6d41\u7a0b\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528 BERT \u7684\u4f8b\u5b50\u3002 import kashgari from kashgari.tasks.labeling import BLSTMModel from kashgari.embeddings import BERTEmbedding bert_embed = BERTEmbedding ( '<PRE_TRAINED_BERT_MODEL_FOLDER>' , task = kashgari . LABELING , sequence_length = 100 ) model = BLSTMModel ( bert_embed ) model . fit ( train_x , train_y , valid_x , valid_y ) \u4f60\u8fd8\u53ef\u4ee5\u628a BERT \u66ff\u6362\u6210 WordEmbedding \u6216\u8005 GPT2Embedding \u7b49\uff0c\u66f4\u591a\u8bf7\u67e5\u9605 Embedding \u6587\u6863 \u8c03\u6574\u6a21\u578b\u8d85\u53c2\u6570 # \u901a\u8fc7\u6a21\u578b\u7684 get_default_hyper_parameters() \u65b9\u6cd5\u53ef\u4ee5\u83b7\u53d6\u9ed8\u8ba4\u8d85\u53c2\uff0c\u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u3002\u901a\u8fc7\u4fee\u6539\u5b57\u5178\u6765\u4fee\u6539\u8d85\u53c2\u5217\u8868\u3002\u518d\u4f7f\u7528\u65b0\u7684\u8d85\u53c2\u5b57\u5178\u521d\u59cb\u5316\u6a21\u578b\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u628a layer_blstm \u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u8c03\u6574\u4e3a 32\uff1a from kashgari.tasks.labeling import BLSTMModel hyper = BLSTMModel . get_default_hyper_parameters () print ( hyper ) # {'layer_blstm': {'units': 128, 'return_sequences': True}, 'layer_dropout': {'rate': 0.4}, 'layer_time_distributed': {}, 'layer_activation': {'activation': 'softmax'}} hyper [ 'layer_blstm' ][ 'units' ] = 32 model = BLSTMModel ( hyper_parameters = hyper ) \u4f7f\u7528\u8bad\u7ec3\u56de\u8c03 # Kashgari \u662f\u57fa\u4e8e tf.keras, \u6240\u4ee5\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u5168\u90e8\u7684 tf.keras \u56de\u8c03\u7c7b \uff0c\u4f8b\u5982\u6211\u4eec\u4f7f\u7528 TensorBoard \u53ef\u89c6\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002 from tensorflow.python import keras from kashgari.tasks.labeling import BLSTMModel from kashgari.callbacks import EvalCallBack model = BLSTMModel () tf_board_callback = keras . callbacks . TensorBoard ( log_dir = './logs' , update_freq = 1000 ) # \u8fd9\u662f Kashgari \u5185\u7f6e\u56de\u8c03\u51fd\u6570\uff0c\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u8ba1\u7b97\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\u548c F1 eval_callback = EvalCallBack ( kash_model = model , valid_x = valid_x , valid_y = valid_y , step = 5 ) model . fit ( train_x , train_y , valid_x , valid_y , batch_size = 100 , callbacks = [ eval_callback , tf_board_callback ]) \u81ea\u5b9a\u4e49\u6a21\u578b\u7ed3\u6784 # \u9664\u4e86\u5185\u7f6e\u6a21\u578b\u4ee5\u5916\uff0c\u8fd8\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u81ea\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u578b\u7ed3\u6784\u3002\u53ea\u9700\u8981\u7ee7\u627f BaseLabelingModel \u5bf9\u8c61\uff0c\u7136\u540e\u5b9e\u73b0 get_default_hyper_parameters() \u65b9\u6cd5 \u548c build_model_arc() \u65b9\u6cd5\u3002 from typing import Dict , Any from tensorflow import keras from kashgari.tasks.labeling.base_model import BaseLabelingModel from kashgari.layers import L import logging logging . basicConfig ( level = 'DEBUG' ) class DoubleBLSTMModel ( BaseLabelingModel ): \"\"\"Bidirectional LSTM Sequence Labeling Model\"\"\" @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Get hyper parameters of model Returns: hyper parameters dict \"\"\" return { 'layer_blstm1' : { 'units' : 128 , 'return_sequences' : True }, 'layer_blstm2' : { 'units' : 128 , 'return_sequences' : True }, 'layer_dropout' : { 'rate' : 0.4 }, 'layer_time_distributed' : {}, 'layer_activation' : { 'activation' : 'softmax' } } def build_model_arc ( self ): \"\"\" build model architectural \"\"\" # \u6b64\u5904\u4f5c\u7528\u662f\u4ece\u4e0a\u5c42\u62ff\u5230\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u548c Embedding \u5c42\u7684\u8f93\u51fa output_dim = len ( self . pre_processor . label2idx ) config = self . hyper_parameters embed_model = self . embedding . embed_model # \u5b9a\u4e49\u4f60\u81ea\u5df1\u7684\u5c42 layer_blstm1 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm1' ]), name = 'layer_blstm1' ) layer_blstm2 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm2' ]), name = 'layer_blstm2' ) layer_dropout = L . Dropout ( ** config [ 'layer_dropout' ], name = 'layer_dropout' ) layer_time_distributed = L . TimeDistributed ( L . Dense ( output_dim , ** config [ 'layer_time_distributed' ]), name = 'layer_time_distributed' ) layer_activation = L . Activation ( ** config [ 'layer_activation' ]) # \u5b9a\u4e49\u6570\u636e\u6d41 tensor = layer_blstm1 ( embed_model . output ) tensor = layer_blstm2 ( tensor ) tensor = layer_dropout ( tensor ) tensor = layer_time_distributed ( tensor ) output_tensor = layer_activation ( tensor ) # \u521d\u59cb\u5316\u6a21\u578b self . tf_model = keras . Model ( embed_model . inputs , output_tensor ) # \u6b64\u6a21\u578b\u53ef\u4ee5\u548c\u4efb\u4f55\u4e00\u4e2a Embedding \u7ec4\u5408\u4f7f\u7528 model = DoubleBLSTMModel () model . fit ( train_x , train_y , valid_x , valid_y ) \u4f7f\u7528 CuDNN \u52a0\u901f GPU \u8bad\u7ec3 # Kashgari \u53ef\u4ee5\u4f7f\u7528 CuDNN \u5c42 \u6765\u52a0\u901f\u8bad\u7ec3\u3002CuDNNLSTM \u548c CuDNNGRU \u8bad\u7ec3\u901f\u5ea6\u6bd4 LSTM \u548c GRU \u5feb\u5f88\u591a\uff0c\u4f46\u662f\u53ea\u80fd\u5728 GPU \u4e0a\u4f7f\u7528\u3002\u5982\u679c\u9700\u8981 GPU \u8bad\u7ec3\uff0cCPU \u63a8\u65ad\uff0c\u90a3\u4e48\u4e0d\u80fd\u4f7f\u7528 CuDNN \u6765\u52a0\u901f\u8bad\u7ec3\u3002\u8bbe\u7f6e CuDNN \u65b9\u6cd5\u5982\u4e0b\uff1a kashgari . config . use_cudnn_cell = True Performance report # Available model list, matrics based on this training: corpus: ChineseDailyNerCorpus epochs: 50 epochs with callbacks batch_size: 64 T4 GPU / 2 CPU / 30 GB on openbayes colab link early_stop = keras . callbacks . EarlyStopping ( patience = 10 ) reduse_lr_callback = keras . callbacks . ReduceLROnPlateau ( factor = 0.1 , patience = 5 ) Name Embedding F1 Score Epoch Time Non Trainable params Trainable params BiLSTM_Model Random Init 0.74147 9.5s 0 558176 BiLSTM CRF Model Random Init 0.81378 123.0s 0 573168 BiGRU_Model Random Init 0.74375 9.7s 0 499296 BiGRU CRF Model Random Init 0.82516 120.7s 0 514288 BiLSTM_Model BERT 0.92727 183.0s 101360640 3280904 BiLSTM CRF Model BERT 0.94013 265.0s 101360640 3295896 BiGRU_Model BERT 0.92700 180.4s 101360640 2461192 BiGRU CRF Model BERT 0.94319 263.4s 101360640 2476184 BiLSTM_Model ERNIE 0.93109 167.6s 98958336 3280904 BiLSTM CRF Model ERNIE 0.94460 250.6s 98958336 3295896 BiGRU_Model ERNIE 0.93512 165.7s 98958336 2461192 BiGRU CRF Model ERNIE 0.94218 250.4s 98958336 2476184","title":"\u6587\u672c\u6807\u6ce8"},{"location":"tutorial/text-labeling/#_1","text":"Kashgari \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\u3002\u6240\u6709\u7684\u6587\u672c\u6807\u6ce8\u6a21\u578b\u90fd\u7ee7\u627f\u81ea BaseLabelingModel \u7c7b\uff0c\u63d0\u4f9b\u4e86\u540c\u6837\u7684 API\u3002\u6240\u4ee5\u5207\u6362\u6a21\u578b\u505a\u5b9e\u9a8c\u975e\u5e38\u7684\u65b9\u4fbf\u3002","title":"\u6587\u672c\u6807\u6ce8"},{"location":"tutorial/text-labeling/#available-models","text":"Name Info CNN_LSTM_Model BiLSTM_Model BiLSTM_CRF_Model BiGRU_Model BiGRU_CRF_Model","title":"Available Models"},{"location":"tutorial/text-labeling/#_2","text":"Kashgari \u5185\u7f6e\u4e86\u4eba\u6c11\u65e5\u62a5\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c CONLL 2003 \u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u65b9\u4fbf\u5feb\u901f\u5b9e\u9a8c\u3002 \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6 # \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6 ## \u4e2d\u6587\u6570\u636e\u96c6 from kashgari.corpus import ChineseDailyNerCorpus train_x , train_y = ChineseDailyNerCorpus . load_data ( 'train' ) valid_x , valid_y = ChineseDailyNerCorpus . load_data ( 'valid' ) test_x , test_y = ChineseDailyNerCorpus . load_data ( 'test' ) ## \u82f1\u6587\u6570\u636e\u96c6 from kashgari.corpus import CONLL2003ENCorpus train_x , train_y = CONLL2003ENCorpus . load_data ( 'train' ) valid_x , valid_y = CONLL2003ENCorpus . load_data ( 'valid' ) test_x , test_y = CONLL2003ENCorpus . load_data ( 'test' ) # \u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\u96c6 train_x = [[ 'Hello' , 'world' ], [ 'Hello' , 'Kashgari' ], [ 'I' , 'love' , 'Beijing' ]] train_y = [[ 'O' , 'O' ], [ 'O' , 'B-PER' ], [ 'O' , 'B-LOC' ]] valid_x , valid_y = train_x , train_y test_x , test_x = train_x , train_y \u9664\u4e86\u4f7f\u7528\u5185\u7f6e\u6570\u636e\u96c6\uff0c\u4f60\u4e5f\u53ef\u4ee5\u52a0\u8f7d\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u6570\u636e\u683c\u5f0f\u548c\u5185\u7f6e\u6570\u636e\u96c6\u4e00\u6837\u5373\u53ef\uff0c\u5efa\u8bae\u6309\u7167 BIO \u89c4\u8303\u8fdb\u884c\u6807\u6ce8\u3002\u5185\u7f6e\u6570\u636e\u96c6\u683c\u5f0f\u5982\u4e0b\uff1a >>> print ( train_x [ 0 ]) [ '\u6d77' , '\u9493' , '\u6bd4' , '\u8d5b' , '\u5730' , '\u70b9' , '\u5728' , '\u53a6' , '\u95e8' , '\u4e0e' , '\u91d1' , '\u95e8' , '\u4e4b' , '\u95f4' , '\u7684' , '\u6d77' , '\u57df' , '\u3002' ] >>> print ( train_y [ 0 ]) [ 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'O' , 'B-LOC' , 'I-LOC' , 'O' , 'B-LOC' , 'I-LOC' , 'O' , 'O' , 'O' , 'O' , 'O' , 'O' ] \u6570\u636e\u51c6\u5907\u597d\u4e86\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u4e86\u3002\u6240\u6709\u7684\u6807\u6ce8\u6a21\u578b\u63d0\u4f9b\u76f8\u540c\u7684 API\uff0c\u6240\u4ee5\u66ff\u6362\u6a21\u578b\u67b6\u6784\u975e\u5e38\u65b9\u4fbf\u3002 import kashgari from kashgari.tasks.labeling import BLSTMModel model = BLSTMModel () model . fit ( train_x , train_y , valid_x , valid_y ) # \u9a8c\u8bc1\u6a21\u578b\uff0c\u6b64\u65b9\u6cd5\u5c06\u6253\u5370\u51fa\u8be6\u7ec6\u7684\u9a8c\u8bc1\u62a5\u544a model . evaluate ( test_x , test_y ) # \u4fdd\u5b58\u6a21\u578b\u5230 `saved_ner_model` \u76ee\u5f55\u4e0b model . save ( 'saved_ner_model' ) # \u52a0\u8f7d\u4fdd\u5b58\u6a21\u578b loaded_model = kashgari . utils . load_model ( 'saved_ner_model' ) # \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b loaded_model . predict ( test_x [: 10 ])","title":"\u8bad\u7ec3\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b"},{"location":"tutorial/text-labeling/#_3","text":"Kashgari \u5185\u7f6e\u4e86\u51e0\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6a21\u5757\uff0c\u7b80\u5316\u8fc1\u79fb\u5b66\u4e60\u6d41\u7a0b\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528 BERT \u7684\u4f8b\u5b50\u3002 import kashgari from kashgari.tasks.labeling import BLSTMModel from kashgari.embeddings import BERTEmbedding bert_embed = BERTEmbedding ( '<PRE_TRAINED_BERT_MODEL_FOLDER>' , task = kashgari . LABELING , sequence_length = 100 ) model = BLSTMModel ( bert_embed ) model . fit ( train_x , train_y , valid_x , valid_y ) \u4f60\u8fd8\u53ef\u4ee5\u628a BERT \u66ff\u6362\u6210 WordEmbedding \u6216\u8005 GPT2Embedding \u7b49\uff0c\u66f4\u591a\u8bf7\u67e5\u9605 Embedding \u6587\u6863","title":"\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60"},{"location":"tutorial/text-labeling/#_4","text":"\u901a\u8fc7\u6a21\u578b\u7684 get_default_hyper_parameters() \u65b9\u6cd5\u53ef\u4ee5\u83b7\u53d6\u9ed8\u8ba4\u8d85\u53c2\uff0c\u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u3002\u901a\u8fc7\u4fee\u6539\u5b57\u5178\u6765\u4fee\u6539\u8d85\u53c2\u5217\u8868\u3002\u518d\u4f7f\u7528\u65b0\u7684\u8d85\u53c2\u5b57\u5178\u521d\u59cb\u5316\u6a21\u578b\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u628a layer_blstm \u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u8c03\u6574\u4e3a 32\uff1a from kashgari.tasks.labeling import BLSTMModel hyper = BLSTMModel . get_default_hyper_parameters () print ( hyper ) # {'layer_blstm': {'units': 128, 'return_sequences': True}, 'layer_dropout': {'rate': 0.4}, 'layer_time_distributed': {}, 'layer_activation': {'activation': 'softmax'}} hyper [ 'layer_blstm' ][ 'units' ] = 32 model = BLSTMModel ( hyper_parameters = hyper )","title":"\u8c03\u6574\u6a21\u578b\u8d85\u53c2\u6570"},{"location":"tutorial/text-labeling/#_5","text":"Kashgari \u662f\u57fa\u4e8e tf.keras, \u6240\u4ee5\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u5168\u90e8\u7684 tf.keras \u56de\u8c03\u7c7b \uff0c\u4f8b\u5982\u6211\u4eec\u4f7f\u7528 TensorBoard \u53ef\u89c6\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002 from tensorflow.python import keras from kashgari.tasks.labeling import BLSTMModel from kashgari.callbacks import EvalCallBack model = BLSTMModel () tf_board_callback = keras . callbacks . TensorBoard ( log_dir = './logs' , update_freq = 1000 ) # \u8fd9\u662f Kashgari \u5185\u7f6e\u56de\u8c03\u51fd\u6570\uff0c\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u8ba1\u7b97\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\u548c F1 eval_callback = EvalCallBack ( kash_model = model , valid_x = valid_x , valid_y = valid_y , step = 5 ) model . fit ( train_x , train_y , valid_x , valid_y , batch_size = 100 , callbacks = [ eval_callback , tf_board_callback ])","title":"\u4f7f\u7528\u8bad\u7ec3\u56de\u8c03"},{"location":"tutorial/text-labeling/#_6","text":"\u9664\u4e86\u5185\u7f6e\u6a21\u578b\u4ee5\u5916\uff0c\u8fd8\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u81ea\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u578b\u7ed3\u6784\u3002\u53ea\u9700\u8981\u7ee7\u627f BaseLabelingModel \u5bf9\u8c61\uff0c\u7136\u540e\u5b9e\u73b0 get_default_hyper_parameters() \u65b9\u6cd5 \u548c build_model_arc() \u65b9\u6cd5\u3002 from typing import Dict , Any from tensorflow import keras from kashgari.tasks.labeling.base_model import BaseLabelingModel from kashgari.layers import L import logging logging . basicConfig ( level = 'DEBUG' ) class DoubleBLSTMModel ( BaseLabelingModel ): \"\"\"Bidirectional LSTM Sequence Labeling Model\"\"\" @classmethod def get_default_hyper_parameters ( cls ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Get hyper parameters of model Returns: hyper parameters dict \"\"\" return { 'layer_blstm1' : { 'units' : 128 , 'return_sequences' : True }, 'layer_blstm2' : { 'units' : 128 , 'return_sequences' : True }, 'layer_dropout' : { 'rate' : 0.4 }, 'layer_time_distributed' : {}, 'layer_activation' : { 'activation' : 'softmax' } } def build_model_arc ( self ): \"\"\" build model architectural \"\"\" # \u6b64\u5904\u4f5c\u7528\u662f\u4ece\u4e0a\u5c42\u62ff\u5230\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u548c Embedding \u5c42\u7684\u8f93\u51fa output_dim = len ( self . pre_processor . label2idx ) config = self . hyper_parameters embed_model = self . embedding . embed_model # \u5b9a\u4e49\u4f60\u81ea\u5df1\u7684\u5c42 layer_blstm1 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm1' ]), name = 'layer_blstm1' ) layer_blstm2 = L . Bidirectional ( L . LSTM ( ** config [ 'layer_blstm2' ]), name = 'layer_blstm2' ) layer_dropout = L . Dropout ( ** config [ 'layer_dropout' ], name = 'layer_dropout' ) layer_time_distributed = L . TimeDistributed ( L . Dense ( output_dim , ** config [ 'layer_time_distributed' ]), name = 'layer_time_distributed' ) layer_activation = L . Activation ( ** config [ 'layer_activation' ]) # \u5b9a\u4e49\u6570\u636e\u6d41 tensor = layer_blstm1 ( embed_model . output ) tensor = layer_blstm2 ( tensor ) tensor = layer_dropout ( tensor ) tensor = layer_time_distributed ( tensor ) output_tensor = layer_activation ( tensor ) # \u521d\u59cb\u5316\u6a21\u578b self . tf_model = keras . Model ( embed_model . inputs , output_tensor ) # \u6b64\u6a21\u578b\u53ef\u4ee5\u548c\u4efb\u4f55\u4e00\u4e2a Embedding \u7ec4\u5408\u4f7f\u7528 model = DoubleBLSTMModel () model . fit ( train_x , train_y , valid_x , valid_y )","title":"\u81ea\u5b9a\u4e49\u6a21\u578b\u7ed3\u6784"},{"location":"tutorial/text-labeling/#cudnn-gpu","text":"Kashgari \u53ef\u4ee5\u4f7f\u7528 CuDNN \u5c42 \u6765\u52a0\u901f\u8bad\u7ec3\u3002CuDNNLSTM \u548c CuDNNGRU \u8bad\u7ec3\u901f\u5ea6\u6bd4 LSTM \u548c GRU \u5feb\u5f88\u591a\uff0c\u4f46\u662f\u53ea\u80fd\u5728 GPU \u4e0a\u4f7f\u7528\u3002\u5982\u679c\u9700\u8981 GPU \u8bad\u7ec3\uff0cCPU \u63a8\u65ad\uff0c\u90a3\u4e48\u4e0d\u80fd\u4f7f\u7528 CuDNN \u6765\u52a0\u901f\u8bad\u7ec3\u3002\u8bbe\u7f6e CuDNN \u65b9\u6cd5\u5982\u4e0b\uff1a kashgari . config . use_cudnn_cell = True","title":"\u4f7f\u7528 CuDNN \u52a0\u901f GPU \u8bad\u7ec3"},{"location":"tutorial/text-labeling/#performance-report","text":"Available model list, matrics based on this training: corpus: ChineseDailyNerCorpus epochs: 50 epochs with callbacks batch_size: 64 T4 GPU / 2 CPU / 30 GB on openbayes colab link early_stop = keras . callbacks . EarlyStopping ( patience = 10 ) reduse_lr_callback = keras . callbacks . ReduceLROnPlateau ( factor = 0.1 , patience = 5 ) Name Embedding F1 Score Epoch Time Non Trainable params Trainable params BiLSTM_Model Random Init 0.74147 9.5s 0 558176 BiLSTM CRF Model Random Init 0.81378 123.0s 0 573168 BiGRU_Model Random Init 0.74375 9.7s 0 499296 BiGRU CRF Model Random Init 0.82516 120.7s 0 514288 BiLSTM_Model BERT 0.92727 183.0s 101360640 3280904 BiLSTM CRF Model BERT 0.94013 265.0s 101360640 3295896 BiGRU_Model BERT 0.92700 180.4s 101360640 2461192 BiGRU CRF Model BERT 0.94319 263.4s 101360640 2476184 BiLSTM_Model ERNIE 0.93109 167.6s 98958336 3280904 BiLSTM CRF Model ERNIE 0.94460 250.6s 98958336 3295896 BiGRU_Model ERNIE 0.93512 165.7s 98958336 2461192 BiGRU CRF Model ERNIE 0.94218 250.4s 98958336 2476184","title":"Performance report"}]}